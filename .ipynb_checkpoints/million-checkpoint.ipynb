{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from scipy.sparse import csr_matrix, vstack, csr_array\n",
    "import time\n",
    "import random\n",
    "import webbrowser\n",
    "import pickle, gzip, joblib, shelve\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, font\n",
    "import threading, time\n",
    "from itertools import islice, combinations\n",
    "from datetime import datetime\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 playlists\n"
     ]
    }
   ],
   "source": [
    "directory_path = 'data/raw'\n",
    "filenames = sorted(os.listdir(directory_path))\n",
    "print(f\"{len(filenames) * 1000} playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# looking at only the first 30,000 playlists\n",
    "fullpaths = [directory_path + '/' + f for f in filenames][0:985]\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34443 songs processed\n",
      "'song_relationships' saved to song_data/00_15_08_2023_song_relationships.gz\n"
     ]
    }
   ],
   "source": [
    "song_relationships = {}\n",
    "iteration_times = []\n",
    "\n",
    "song_uris_set = set()\n",
    "\n",
    "for idx, path in enumerate(fullpaths):\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(path) as f:\n",
    "        mpd_slice = json.load(f)\n",
    "\n",
    "    playlists_data = mpd_slice['playlists']\n",
    "\n",
    "    for playlist in playlists_data:\n",
    "        songs_set = set()\n",
    "\n",
    "        for track in playlist['tracks']:\n",
    "            song_uri = track['track_uri'].split(':')[-1]            \n",
    "            songs_set.add(song_uri)\n",
    "\n",
    "        # Compute song-to-song relationships for the current playlist\n",
    "        pair_counts = Counter(combinations(songs_set, 2))\n",
    "        \n",
    "        for (song1, song2), count in pair_counts.items():\n",
    "            song_relationships.setdefault(song1, {}).setdefault(song2, 0)\n",
    "            song_relationships[song1][song2] += count\n",
    "            song_relationships.setdefault(song2, {}).setdefault(song1, 0)\n",
    "            song_relationships[song2][song1] += count\n",
    "\n",
    "    end_time = time.time()\n",
    "    iteration_time = end_time - start_time\n",
    "    iteration_times.append(iteration_time)\n",
    "\n",
    "    if idx % 10 == 0 and idx > 0:\n",
    "        t = np.sum(iteration_times[-10:])\n",
    "        print(f\"processing {idx - 10}-{idx} - time taken {t:.2f}\")\n",
    "\n",
    "print(f'{len(song_relationships)} songs processed')\n",
    "\n",
    "if save:\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "\n",
    "    save_path = os.path.join('song_data', f'{formatted_time}_song_relationships.gz')\n",
    "\n",
    "    with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(song_relationships, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"'song_relationships' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to load song_relationships from storage\n",
    "\n",
    "save_path = os.path.join('song_data', '18_14_08_2023_song_relationships.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    song_relationships = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34443 songs processed\n",
      "'song_data_map' saved to song_data/00_15_08_2023_song_data_map.gz\n"
     ]
    }
   ],
   "source": [
    "song_data_map = {}\n",
    "for idx, path in enumerate(fullpaths):\n",
    "    if idx % 100 == 0 and idx > 0:\n",
    "        print(f\"Processed {idx-100}-{idx}\")\n",
    "    with open(path) as f:\n",
    "        mpd_slice = json.load(f)\n",
    "    playlists_data = mpd_slice['playlists']\n",
    "    for playlist in playlists_data:\n",
    "        for track in playlist['tracks']:\n",
    "            song_uri = track['track_uri'].split(':')[-1]\n",
    "            song_name = track['track_name']\n",
    "            album_name = track['album_name']\n",
    "            artist_name = track['artist_name']\n",
    "            artist_uri = track['artist_uri']\n",
    "            album_uri = track['album_uri']\n",
    "            if song_uri in song_relationships:\n",
    "                song_data_map[song_uri] = {'song_name': song_name, 'album_name': album_name, 'artist_name': artist_name,\n",
    "                                          'artist_uri': artist_uri, 'album_uri': album_uri}\n",
    "                \n",
    "print(f'{len(song_data_map)} songs processed')\n",
    "\n",
    "if save:\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "    save_path = os.path.join('song_data', f'{formatted_time}_song_data_map.gz')\n",
    "\n",
    "    with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(song_data_map, f)\n",
    "    print(f\"'song_data_map' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load song_data_map from storage\n",
    "save_path = os.path.join('song_data', f'20_14_08_2023_song_data_map.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    song_data_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2244869"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(song_data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'song_indices' saved to song_data/00_15_08_2023_song_indices.gz\n",
      "34443\n"
     ]
    }
   ],
   "source": [
    "song_indices = {song_uri: idx for idx, song_uri in enumerate(song_relationships.keys())}\n",
    "\n",
    "if save:\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "    save_path = os.path.join('song_data', f'{formatted_time}_song_indices.gz')\n",
    "    with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(song_indices, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"'song_indices' saved to {save_path}\")\n",
    "\n",
    "num_songs = len(song_indices)\n",
    "\n",
    "print(num_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244869\n"
     ]
    }
   ],
   "source": [
    "# to load song_indices from storage\n",
    "save_path = os.path.join('song_data', f'20_14_08_2023_song_indices.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    song_indices = pickle.load(f)\n",
    "\n",
    "num_songs = len(song_indices)\n",
    "print(num_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song with the most connections: {'song_name': 'Closer', 'album_name': 'Closer', 'artist_name': 'The Chainsmokers', 'artist_uri': 'spotify:artist:69GGBxA162lTqCwzJG5jLp', 'album_uri': 'spotify:album:0rSLgV8p5FzfnqlEk4GzxE'}\n",
      "Number of connections: 203549\n"
     ]
    }
   ],
   "source": [
    "max_connections = 0\n",
    "song_with_most_connections = None\n",
    "\n",
    "for song, connections in song_relationships.items():\n",
    "    num_connections = len(connections)\n",
    "    if num_connections > max_connections:\n",
    "        max_connections = num_connections\n",
    "        song_with_most_connections = song\n",
    "\n",
    "print(\"Song with the most connections:\", song_data_map[song_with_most_connections])\n",
    "print(\"Number of connections:\", max_connections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     end_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_index \u001b[38;5;241m+\u001b[39m chunk_size, num_songs)\n\u001b[1;32m     37\u001b[0m     current_relationships \u001b[38;5;241m=\u001b[39m slice_dict(song_relationships, start_index, end_index)\n\u001b[0;32m---> 38\u001b[0m     cooccurrence_matrix \u001b[38;5;241m=\u001b[39m update_matrix(full_matrix, current_relationships)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_index\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mchunk_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_chunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Now, the full_matrix is your final cooccurrence_matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "def slice_dict(d, start, end):\n",
    "    return dict(islice(d.items(), start, end))\n",
    "\n",
    "def update_matrix(matrix, chunk):\n",
    "    data = []\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "\n",
    "    for song_uri, relationships in chunk.items():\n",
    "        row_idx = song_indices[song_uri]\n",
    "        for related_song_uri, count in relationships.items():\n",
    "            col_idx = song_indices[related_song_uri]\n",
    "            \n",
    "            if row_idx <= col_idx:\n",
    "                data.append(count)\n",
    "                row_indices.append(row_idx)\n",
    "                col_indices.append(col_idx)\n",
    "                \n",
    "                if row_idx != col_idx:\n",
    "                    data.append(count)\n",
    "                    row_indices.append(col_idx)\n",
    "                    col_indices.append(row_idx)\n",
    "\n",
    "    # Create a temporary csr_matrix\n",
    "    temp_matrix = csr_matrix((data, (row_indices, col_indices)), shape=(num_songs, num_songs), dtype=np.int32)\n",
    "    \n",
    "    return matrix + temp_matrix\n",
    "\n",
    "cooccurrence_matrix = csr_matrix((num_songs, num_songs), dtype=np.int32)\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "num_chunks = (num_songs + chunk_size - 1) // chunk_size\n",
    "\n",
    "for start_index in range(0, num_songs, chunk_size):\n",
    "    end_index = min(start_index + chunk_size, num_songs)\n",
    "    current_relationships = slice_dict(song_relationships, start_index, end_index)\n",
    "    cooccurrence_matrix = update_matrix(full_matrix, current_relationships)\n",
    "    print(f\"processed chunk {start_index//chunk_size + 1}/{num_chunks}\")\n",
    "\n",
    "# Now, the full_matrix is your final cooccurrence_matrix\n",
    "\n",
    "print(f'finished processing matrix size: {cooccurrence_matrix.shape}')\n",
    "\n",
    "try:\n",
    "    with h5py.File('cooccurrence_matrix.h5', 'w') as hf:\n",
    "        hf.create_dataset('cooccurrence_matrix', data=cooccurrence_matrix.toarray(), compression='gzip', compression_opts=9)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if save:\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "    save_path = os.path.join('song_data', f'{formatted_time}_cooccurrence_matrix.gz')\n",
    "    with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(cooccurrence_matrix, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"'cooccurrence_matrix' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity(csr_matrix):\n",
    "    total_elements = csr_matrix.shape[0] * csr_matrix.shape[1]\n",
    "    non_zero_elements = csr_matrix.nnz\n",
    "    sparsity = (total_elements - non_zero_elements) / total_elements\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load cooccurrence_matrix from storage\n",
    "save_path = os.path.join('song_data', f'00_15_08_2023_cooccurrence_matrix.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    cooccurrence_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rows 0 to 5000\n",
      "Processing rows 5000 to 10000\n",
      "Processing rows 10000 to 15000\n",
      "Processing rows 15000 to 20000\n",
      "Processing rows 20000 to 25000\n",
      "Processing rows 25000 to 30000\n",
      "Processing rows 30000 to 34443\n",
      "Finished processing matrix size: (34443, 34443)\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "total_occurrences = np.sum(cooccurrence_matrix)\n",
    "p_i = np.sum(cooccurrence_matrix, axis=1) / total_occurrences\n",
    "p_i = np.asarray(p_i).flatten()\n",
    "p_ij = cooccurrence_matrix / total_occurrences\n",
    "\n",
    "def compute_pmi_for_chunk(start_row, end_row):\n",
    "    # Lists to store data for this chunk\n",
    "    pmi_data_chunk = []\n",
    "    row_indices_chunk = []\n",
    "    col_indices_chunk = []\n",
    "\n",
    "    for i in range(start_row, end_row):\n",
    "        for data_idx in range(p_ij.indptr[i], p_ij.indptr[i + 1]):\n",
    "            j = p_ij.indices[data_idx]\n",
    "\n",
    "            if p_ij.data[data_idx] > 0:  # Avoid log(0)\n",
    "                original_pmi = np.log2(p_ij.data[data_idx] / (p_i[i] * p_i[j]))\n",
    "                pmi_score = original_pmi #- (-(k - 1) * np.log2(p_ij.data[data_idx]))\n",
    "                if pmi_score > 0:\n",
    "                    pmi_data_chunk.append(pmi_score)\n",
    "                    row_indices_chunk.append(i - start_row)  # Adjust the row index relative to the chunk\n",
    "                    col_indices_chunk.append(j)\n",
    "                \n",
    "    return pmi_data_chunk, row_indices_chunk, col_indices_chunk\n",
    "\n",
    "if save:\n",
    "    # Define chunk size\n",
    "    chunk_size = 5000\n",
    "    saved_chunk_files = []\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "\n",
    "    for start in range(0, cooccurrence_matrix.shape[0], chunk_size):\n",
    "        end = min(start + chunk_size, cooccurrence_matrix.shape[0])\n",
    "        print(f\"Processing rows {start} to {end}\")\n",
    "        \n",
    "        data, row, col = compute_pmi_for_chunk(start, end)\n",
    "        chunk_matrix = csr_matrix((data, (row, col)), shape=(end-start, cooccurrence_matrix.shape[1]), dtype=np.float64)\n",
    "\n",
    "        save_path = os.path.join('song_data/pmi', f'{formatted_time}_{start}_{end}_{k}_pmi_matrix.gz')\n",
    "        with gzip.open(save_path, 'wb') as f:\n",
    "            pickle.dump(chunk_matrix, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        saved_chunk_files.append(save_path)\n",
    "\n",
    "    # Combining saved chunks\n",
    "    chunks = []\n",
    "    for chunk_file in saved_chunk_files:\n",
    "        with gzip.open(chunk_file, 'rb') as f:\n",
    "            chunks.append(pickle.load(f))\n",
    "\n",
    "    pmi_matrix = vstack(chunks, format='csr')\n",
    "    print(f'Finished processing matrix size: {pmi_matrix.shape}')\n",
    "\n",
    "else:\n",
    "    chunk_size = 5000\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, cooccurrence_matrix.shape[0], chunk_size):\n",
    "        end = min(start + chunk_size, cooccurrence_matrix.shape[0])\n",
    "        print(f\"Processing rows {start} to {end}\")\n",
    "        \n",
    "        data, row, col = compute_pmi_for_chunk(start, end)\n",
    "        chunk_matrix = csr_matrix((data, (row, col)), shape=(end-start, cooccurrence_matrix.shape[1]), dtype=np.float64)\n",
    "        chunks.append(chunk_matrix)\n",
    "        \n",
    "    pmi_matrix = vstack(chunks, format='csr')\n",
    "    print(f'Finished processing matrix size: {pmi_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load pmi_matrix from storage\n",
    "\n",
    "folder_path = 'song_data/pmi'  # Replace with the path to your folder\n",
    "saved_chunk_files = [folder_path + '/' + f for f in os.listdir(folder_path) if \"16_13_08_2023\" in f]\n",
    "saved_chunk_files = sorted(saved_chunk_files, key = lambda x: int(x.split('_')[5]))\n",
    "chunks = []\n",
    "for chunk_file in saved_chunk_files:\n",
    "    with gzip.open(chunk_file, 'rb') as f:\n",
    "        chunks.append(pickle.load(f))\n",
    "\n",
    "pmi_matrix = vstack(chunks, format='csr')\n",
    "print(f'Finished processing matrix size: {pmi_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_playlist_vector(playlist_songs, song_indices, num_songs):\n",
    "    if not playlist_songs:\n",
    "        return csr_matrix(np.ones((1, num_songs)))\n",
    "    \n",
    "    indices = [song_indices[song] for song in playlist_songs]\n",
    "    data = np.ones(len(indices))\n",
    "    indptr = np.array([0, len(indices)])\n",
    "    return csr_matrix((data, indices, indptr), shape=(1, num_songs))\n",
    "\n",
    "def compute_scores(user_vector, pmi_matrix):\n",
    "    s = time.time()\n",
    "    scores = user_vector @ pmi_matrix\n",
    "    e = time.time()\n",
    "    print(e - s)\n",
    "    return scores.toarray()[0]\n",
    "\n",
    "# def get_top_recommendations(scores, song_data_map, song_indices, n=10):\n",
    "#     s = time.time()\n",
    "#     top_indices = np.argsort(scores)[-n:][::-1]\n",
    "#     top_songs = [song_data_map[list(song_indices)[i]] for i in top_indices]\n",
    "#     e = time.time()\n",
    "#     print(e - s)\n",
    "#     return top_songs\n",
    "\n",
    "def get_top_recommendations(scores, song_data_map, song_indices, n=10):\n",
    "    s = time.time()\n",
    "    \n",
    "    # Get the top n indices without sorting the entire array\n",
    "    top_indices = np.argpartition(scores, -n)[-n:]\n",
    "    # Now, sort only the top n indices\n",
    "    top_indices_sorted = top_indices[np.argsort(scores[top_indices])][::-1]\n",
    "    \n",
    "    song_indices_list = list(song_indices.keys())\n",
    "    top_songs = [song_data_map[song_indices_list[i]] for i in top_indices_sorted]\n",
    "    \n",
    "    e = time.time()\n",
    "    print(e - s)\n",
    "    return top_songs\n",
    "\n",
    "def recommend_songs_pmi(user_playlist, song_indices, pmi_matrix, song_data_map, n=10):\n",
    "    user_vector = user_playlist_vector(user_playlist, song_indices, num_songs)\n",
    "    scores = compute_scores(user_vector, pmi_matrix)\n",
    "    return get_top_recommendations(scores, song_data_map, song_indices, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongRecommendationApp(tk.Tk):\n",
    "    def __init__(self, song_data_map):\n",
    "        super().__init__()\n",
    "\n",
    "        default_font = font.nametofont(\"TkDefaultFont\")\n",
    "        default_font.configure(family=\"Courier\")\n",
    "\n",
    "        self.song_data_map = song_data_map\n",
    "        self.uri_map = {self.format_song_display(song_info): uri for uri, song_info in song_data_map.items()}\n",
    "        self.playlist_data = []  # Store song data for sorting\n",
    "\n",
    "        # Filter Frame\n",
    "        self.filter_frame = ttk.Frame(self)\n",
    "        self.filter_frame.pack(pady=10)\n",
    "\n",
    "        # Label and Entry for Song\n",
    "        self.song_label = ttk.Label(self.filter_frame, text=\"Song\")\n",
    "        self.song_label.grid(row=0, column=0, padx=5)\n",
    "        self.song_entry = ttk.Entry(self.filter_frame)\n",
    "        self.song_entry.grid(row=1, column=0, padx=5)\n",
    "\n",
    "        # Label and Entry for Artist\n",
    "        self.artist_label = ttk.Label(self.filter_frame, text=\"Artist\")\n",
    "        self.artist_label.grid(row=0, column=1, padx=5)\n",
    "        self.artist_entry = ttk.Entry(self.filter_frame)\n",
    "        self.artist_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "        # Label and Entry for Album\n",
    "        self.album_label = ttk.Label(self.filter_frame, text=\"Album\")\n",
    "        self.album_label.grid(row=0, column=2, padx=5)\n",
    "        self.album_entry = ttk.Entry(self.filter_frame)\n",
    "        self.album_entry.grid(row=1, column=2, padx=5)\n",
    "\n",
    "        # Debounce logic\n",
    "        self.last_time = time.time()\n",
    "\n",
    "        self.search_button = ttk.Button(self.filter_frame, text=\"Search\", command=self.display_search_results)\n",
    "        self.search_button.grid(row=2, columnspan=3, pady=10)\n",
    "\n",
    "        width = 200\n",
    "        # Songs Listbox\n",
    "        self.songs_listbox = tk.Listbox(self, selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.songs_listbox.pack(pady=10)\n",
    "\n",
    "        # Drag & Drop functionality\n",
    "        self.songs_listbox.bind('<<ListboxSelect>>', self.add_to_playlist)\n",
    "\n",
    "        # Playlist Listbox\n",
    "        self.playlist_listbox = tk.Listbox(self, bg=\"lightblue\", selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.playlist_listbox.pack(pady=10)\n",
    "\n",
    "        # Number of recommendations\n",
    "        self.n_label = ttk.Label(self, text=\"Number of Recommendations:\")\n",
    "        self.n_label.pack(pady=5)\n",
    "        self.n_entry = ttk.Entry(self)\n",
    "        self.n_entry.pack(pady=5)\n",
    "\n",
    "        # Button to generate recommendations\n",
    "        self.btn_recommend = ttk.Button(self, text=\"Generate Recommendations\", command=self.generate_recommendations)\n",
    "        self.btn_recommend.pack(pady=10)\n",
    "\n",
    "        # Recommendations Listbox\n",
    "        self.recommendations_listbox = tk.Listbox(self, bg=\"lightgreen\", selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.recommendations_listbox.pack(pady=10)\n",
    "\n",
    "        self.recommendations_listbox.bind('<Double-Button-1>', self.open_in_spotify)\n",
    "\n",
    "        self.btn_refresh = ttk.Button(self, text=\"Refresh\", command=self.refresh)\n",
    "        self.btn_refresh.pack(pady=10)\n",
    "    \n",
    "    def refresh(self):\n",
    "        # Clear all fields\n",
    "        self.song_entry.delete(0, tk.END)\n",
    "        self.artist_entry.delete(0, tk.END)\n",
    "        self.album_entry.delete(0, tk.END)\n",
    "        self.n_entry.delete(0, tk.END)\n",
    "        \n",
    "        # Clear listboxes\n",
    "        self.songs_listbox.delete(0, tk.END)\n",
    "        self.playlist_listbox.delete(0, tk.END)\n",
    "        self.recommendations_listbox.delete(0, tk.END)\n",
    "\n",
    "    def open_in_spotify(self, event):\n",
    "            selected_index = self.recommendations_listbox.curselection()\n",
    "            if selected_index:\n",
    "                selected_song = self.recommendations_listbox.get(selected_index)\n",
    "                song_uri = self.uri_map[selected_song]\n",
    "                webbrowser.open(f\"https://open.spotify.com/track/{song_uri}\")\n",
    "                \n",
    "    def display_search_results(self):\n",
    "        song_query = self.song_entry.get().lower()\n",
    "        artist_query = self.artist_entry.get().lower()\n",
    "        album_query = self.album_entry.get().lower()\n",
    "\n",
    "        self.songs_listbox.delete(0, tk.END)\n",
    "        results = []  # Store the filtered results first\n",
    "\n",
    "        for uri, song_info in self.song_data_map.items():\n",
    "            if song_query in song_info['song_name'].lower() and artist_query in song_info['artist_name'].lower() and album_query in song_info['album_name'].lower():\n",
    "                display_name = self.format_song_display(song_info)\n",
    "                results.append(display_name)\n",
    "\n",
    "        # Sort by album name\n",
    "        results.sort(key=lambda x: self.song_data_map[self.uri_map[x]]['album_name'])\n",
    "\n",
    "        # Display the sorted results\n",
    "        for display_name in results:\n",
    "            self.songs_listbox.insert(tk.END, display_name)\n",
    "\n",
    "        if len(results) > 300:  # If you want to limit the displayed results\n",
    "            self.songs_listbox.delete(301, tk.END)\n",
    "\n",
    "    def format_song_display(self, song_info):\n",
    "        formatted_str = \"{:<65}{:<35}{:<35}\"\n",
    "        f_string = formatted_str.format(song_info['song_name'], song_info['artist_name'], song_info['album_name'])\n",
    "        return f_string\n",
    "\n",
    "    def add_to_playlist(self, event):\n",
    "        selected_index = self.songs_listbox.curselection()\n",
    "        if selected_index:  # This checks if there's any selection at all\n",
    "            selected_song = self.songs_listbox.get(selected_index)\n",
    "            if selected_song not in self.playlist_listbox.get(0, tk.END):  # Prevent duplicates\n",
    "                self.playlist_listbox.insert(tk.END, selected_song)\n",
    "\n",
    "    def generate_recommendations(self):\n",
    "        s = time.time()\n",
    "        playlist_display_names = list(self.playlist_listbox.get(0, tk.END))\n",
    "        playlist_uris = [self.uri_map[display_name] for display_name in playlist_display_names]  # Extract URIs\n",
    "\n",
    "        n = int(self.n_entry.get())\n",
    "        recommended_songs = recommend_songs_pmi(playlist_uris, song_indices, pmi_matrix, song_data_map, n)\n",
    "\n",
    "        self.recommendations_listbox.delete(0, tk.END)\n",
    "        for song in recommended_songs:\n",
    "            formatted_song = self.format_song_display(song)\n",
    "            self.recommendations_listbox.insert(tk.END, formatted_song)\n",
    "        e = time.time()\n",
    "        print(e - s)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = SongRecommendationApp(song_data_map)\n",
    "    app.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
