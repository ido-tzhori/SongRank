{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import time\n",
    "import random\n",
    "import webbrowser\n",
    "import pickle, gzip, joblib, shelve\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, font\n",
    "import threading, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 playlists\n"
     ]
    }
   ],
   "source": [
    "directory_path = 'data/raw'\n",
    "filenames = sorted(os.listdir(directory_path))\n",
    "print(f\"{len(filenames) * 1000} playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# looking at only the first 30,000 playlists\n",
    "fullpaths = [directory_path + '/' + f for f in filenames][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_tracks_per_playlist = 5\n",
    "max_tracks_per_playlist = 150\n",
    "min_albums_per_playlist = 2\n",
    "min_artists_per_playlist = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# song_relationships = {}\n",
    "\n",
    "# t = 0\n",
    "# iteration_times = []\n",
    "\n",
    "# for idx, path in enumerate(fullpaths):\n",
    "#     start_time = time.time()\n",
    "#     if idx % 5 == 0 and idx > 0:\n",
    "#         print(f\"processed {idx-5}-{idx} - time taken {t:.2f}\")\n",
    "#         iteration_times.append(t)\n",
    "#         t = 0\n",
    "#     with open(path) as f:\n",
    "#         mpd_slice = json.load(f)\n",
    "#     playlists_data = mpd_slice['playlists']\n",
    "\n",
    "#     for idx, playlist in enumerate(playlists_data):\n",
    "#         # r = playlist['num_albums'] / playlist['num_tracks']\n",
    "#         # if r < 0.0:\n",
    "#         #     continue\n",
    "#         songs = set(track['track_uri'] for track in playlist['tracks'])\n",
    "#         t_per_p = len(songs)\n",
    "#         albums = set(track['album_uri'] for track in playlist['tracks'])\n",
    "#         alb_per_p = len(albums)\n",
    "#         artists = set(track['artist_uri'] for track in playlist['tracks'])\n",
    "#         art_per_p = len(artists)\n",
    "\n",
    "#         if (min_tracks_per_playlist >= t_per_p) or \\\n",
    "#              (t_per_p >= max_tracks_per_playlist) or \\\n",
    "#                 (min_albums_per_playlist >= alb_per_p) or \\\n",
    "#                     (min_artists_per_playlist >= art_per_p):\n",
    "#             continue\n",
    "\n",
    "#         for song in songs:\n",
    "#             if song not in song_relationships:\n",
    "#                 song_relationships[song] = {}\n",
    "                \n",
    "#             for related_song in songs:\n",
    "#                 if related_song != song:  # avoid self-relationship\n",
    "#                     if related_song in song_relationships[song]:\n",
    "#                         song_relationships[song][related_song] += 1\n",
    "#                     else:\n",
    "#                         song_relationships[song][related_song] = 1\n",
    "#     end_time = time.time()\n",
    "#     t += end_time - start_time\n",
    "\n",
    "# print(f'Number of songs processed: {len(song_relationships)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0-10 - time taken 17.49\n",
      "processed 10-20 - time taken 20.66\n",
      "processed 20-30 - time taken 22.23\n",
      "processed 30-40 - time taken 22.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 371844/371844 [00:47<00:00, 7892.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371844 songs processed\n"
     ]
    }
   ],
   "source": [
    "song_relationships = {}\n",
    "\n",
    "t = 0\n",
    "iteration_times = []\n",
    "\n",
    "for idx, path in enumerate(fullpaths):\n",
    "    start_time = time.time()\n",
    "    if idx % 10 == 0 and idx > 0:\n",
    "        print(f\"processed {idx-10}-{idx} - time taken {t:.2f}\")\n",
    "        iteration_times.append(t)\n",
    "        t = 0\n",
    "    with open(path) as f:\n",
    "        mpd_slice = json.load(f)\n",
    "    playlists_data = mpd_slice['playlists']\n",
    "\n",
    "    for idx, playlist in enumerate(playlists_data):\n",
    "\n",
    "        ## TO DO: remove redundnat info in song_name\n",
    "        songs = set(track['track_uri'] for track in playlist['tracks'])\n",
    "        t_per_p = len(songs)\n",
    "        albums = set(track['album_uri'] for track in playlist['tracks'])\n",
    "        alb_per_p = len(albums)\n",
    "        artists = set(track['artist_uri'] for track in playlist['tracks'])\n",
    "        art_per_p = len(artists)\n",
    "\n",
    "        if (min_tracks_per_playlist >= t_per_p) or \\\n",
    "             (t_per_p >= max_tracks_per_playlist) or \\\n",
    "                (min_albums_per_playlist >= alb_per_p) or \\\n",
    "                    (min_artists_per_playlist >= art_per_p):\n",
    "            continue\n",
    "        \n",
    "        temp_relationships = {}\n",
    "\n",
    "        for song in songs:\n",
    "            if song not in temp_relationships:\n",
    "                temp_relationships[song] = {}\n",
    "            \n",
    "            for related_song in songs:\n",
    "                if related_song != song:  # avoid self-relationship\n",
    "                    if related_song in temp_relationships[song]:\n",
    "                        temp_relationships[song][related_song] += 1\n",
    "                    else:\n",
    "                        temp_relationships[song][related_song] = 1\n",
    "\n",
    "        for song, relations in temp_relationships.items():\n",
    "            if song not in song_relationships:\n",
    "                song_relationships[song] = dict(relations)\n",
    "            else:\n",
    "                for related_song, count in relations.items():\n",
    "                    if related_song in song_relationships[song]:\n",
    "                        song_relationships[song][related_song] += count\n",
    "                    else:\n",
    "                        song_relationships[song][related_song] = count\n",
    "\n",
    "    end_time = time.time()\n",
    "    t += end_time - start_time\n",
    "\n",
    "song_relationships = {key: list(value.items()) for key, value in tqdm(song_relationships.items())}\n",
    "    \n",
    "print(f'{len(song_relationships)} songs processed')\n",
    "\n",
    "# save_path = os.path.join('song_data', 'song_relationships.gz')\n",
    "\n",
    "# with gzip.open(save_path, 'wb') as f:\n",
    "#     pickle.dump(song_relationships, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "# print(f\"'song_relationships' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load song_relationships from storage\n",
    "\n",
    "save_path = os.path.join('song_data', 'song_relationships.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    song_relationships = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0-10\n",
      "Processed 10-20\n",
      "Processed 20-30\n",
      "Processed 30-40\n",
      "'song_data_map' saved to song_data/song_data_map.gz\n"
     ]
    }
   ],
   "source": [
    "song_data_map = {}\n",
    "for idx, path in enumerate(fullpaths):\n",
    "    if idx % 10 == 0 and idx > 0:\n",
    "        print(f\"Processed {idx-10}-{idx}\")\n",
    "    with open(path) as f:\n",
    "        mpd_slice = json.load(f)\n",
    "    playlists_data = mpd_slice['playlists']\n",
    "    for playlist in playlists_data:\n",
    "        for track in playlist['tracks']:\n",
    "            song_uri = track['track_uri']\n",
    "            song_name = track['track_name']\n",
    "            album_name = track['album_name']\n",
    "            artist_name = track['artist_name']\n",
    "            if song_uri in song_relationships:\n",
    "                song_data_map[song_uri] = {'song_name': song_name, 'album_name': album_name, 'artist_name': artist_name}\n",
    "\n",
    "save_path = os.path.join('song_data', 'song_data_map.gz')\n",
    "\n",
    "with gzip.open(save_path, 'wb') as f:\n",
    "    pickle.dump(song_data_map, f)\n",
    "print(f\"'song_data_map' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load song_data_map from storage\n",
    "\n",
    "save_path = os.path.join('song_data', 'song_data_map.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    song_data_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song with the most connections: {'song_name': 'Closer', 'album_name': 'Closer', 'artist_name': 'The Chainsmokers'}\n",
      "Number of connections: 25967\n"
     ]
    }
   ],
   "source": [
    "max_connections = 0\n",
    "song_with_most_connections = None\n",
    "\n",
    "for song, connections in song_relationships.items():\n",
    "    num_connections = len(connections)\n",
    "    if num_connections > max_connections:\n",
    "        max_connections = num_connections\n",
    "        song_with_most_connections = song\n",
    "\n",
    "print(\"Song with the most connections:\", song_data_map[song_with_most_connections])\n",
    "print(\"Number of connections:\", max_connections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_indices = {song_uri: idx for idx, song_uri in enumerate(song_relationships.keys())}\n",
    "num_songs = len(song_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# row_indices = []\n",
    "# col_indices = []\n",
    "# data_sum = []\n",
    "# data = []\n",
    "\n",
    "# for idx, (song_uri, relationships) in enumerate(song_relationships.items()):\n",
    "#     row_idx = song_indices[song_uri]\n",
    "#     # sum_connections = np.sum([x[1] for x in relationships])\n",
    "#     for related_song_uri, count in relationships:\n",
    "#         col_idx = song_indices[related_song_uri]\n",
    "#         row_indices.append(row_idx)\n",
    "#         col_indices.append(col_idx)\n",
    "#         # data_sum.append(count / sum_connections)\n",
    "#         data.append(count)\n",
    "\n",
    "#     if idx % 30000 == 0 and idx > 0:\n",
    "#         print(f\"Processed {idx} rows.....\")\n",
    "\n",
    "# # transition_matrix = csr_matrix((data_sum, (row_indices, col_indices)), shape=(num_songs, num_songs), dtype=np.float64)\n",
    "# cooccurrence_matrix = csr_matrix((data, (row_indices, col_indices)), shape=(num_songs, num_songs), dtype=np.int32)\n",
    "\n",
    "# save_path = os.path.join('song_data', 'cooccurrence_matrix.gz')\n",
    "# with gzip.open(save_path, 'wb') as f:\n",
    "#     pickle.dump(cooccurrence_matrix, f)\n",
    "# print(f\"'cooccurrence_matrix' saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1 and saved to song_data/interim_matrix_0.gz\n",
      "Processed chunk 2 and saved to song_data/interim_matrix_1.gz\n",
      "Processed chunk 3 and saved to song_data/interim_matrix_2.gz\n",
      "Processed chunk 4 and saved to song_data/interim_matrix_3.gz\n",
      "Processed chunk 5 and saved to song_data/interim_matrix_4.gz\n"
     ]
    }
   ],
   "source": [
    "def process_chunk(chunk):\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data = []\n",
    "\n",
    "    for song_uri, relationships in chunk.items():\n",
    "        row_idx = song_indices[song_uri]\n",
    "        for related_song_uri, count in relationships:\n",
    "            col_idx = song_indices[related_song_uri]\n",
    "            \n",
    "            # Only consider the upper triangle of the matrix\n",
    "            if row_idx <= col_idx:\n",
    "                row_indices.append(row_idx)\n",
    "                col_indices.append(col_idx)\n",
    "                data.append(count)\n",
    "                \n",
    "                # If it's not on the diagonal, add the symmetric entry\n",
    "                if row_idx != col_idx:\n",
    "                    row_indices.append(col_idx)\n",
    "                    col_indices.append(row_idx)\n",
    "                    data.append(count)\n",
    "\n",
    "    return csr_matrix((data, (row_indices, col_indices)), shape=(num_songs, num_songs), dtype=np.int32)\n",
    "\n",
    "\n",
    "def process_chunk_on_the_fly(start_index, end_index):\n",
    "    current_relationships = dict(list(song_relationships.items())[start_index:end_index])\n",
    "    return process_chunk(current_relationships)\n",
    "\n",
    "chunk_size = 5000\n",
    "interim_matrices = []\n",
    "\n",
    "for start_index in range(0, num_songs, chunk_size):\n",
    "    end_index = min(start_index + chunk_size, num_songs)\n",
    "    \n",
    "    interim_matrix = process_chunk_on_the_fly(start_index, end_index)\n",
    "    interim_path = os.path.join('song_data', f'interim_matrix_{start_index//chunk_size}.gz')\n",
    "    \n",
    "    with gzip.open(interim_path, 'wb') as f:\n",
    "        pickle.dump(interim_matrix, f)\n",
    "    \n",
    "    interim_matrices.append(interim_path)\n",
    "    print(f\"Processed chunk {start_index//chunk_size + 1} and saved to {interim_path}\")\n",
    "\n",
    "# Combining interim results to get the final matrix\n",
    "cooccurrence_matrix = vstack([pickle.load(gzip.open(path, 'rb')) for path in interim_matrices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 2, 1, 1, 1],\n",
       "       [1, 1, 0, 1, 1, 1, 1, 1, 2, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 0, 1, 2, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 0, 2, 1, 3, 2],\n",
       "       [1, 2, 1, 1, 2, 2, 0, 1, 2, 2],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 1, 2],\n",
       "       [1, 1, 2, 1, 1, 3, 2, 1, 0, 2],\n",
       "       [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrence_matrix[0:10,0:10].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 2, 1, 1, 1],\n",
       "       [1, 1, 0, 1, 1, 1, 1, 1, 2, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 0, 1, 2, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 0, 2, 1, 3, 2],\n",
       "       [1, 2, 1, 1, 2, 2, 0, 1, 2, 2],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 1, 2],\n",
       "       [1, 1, 2, 1, 1, 3, 2, 1, 0, 2],\n",
       "       [1, 1, 1, 1, 1, 2, 2, 2, 2, 0]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrence_matrix[0:10,0:10].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load cooccurrence_matrix from storage\n",
    "\n",
    "save_path = os.path.join('song_data', 'cooccurrence_matrix.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    cooccurrence_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n = 10\n",
    "\n",
    "# popularity = np.sum(transition_matrix, axis=0)\n",
    "# top_n_indices = np.argsort(popularity.A1)[-n:]\n",
    "\n",
    "# init = np.random.rand(1, num_songs)\n",
    "# init = init / np.sum(init)\n",
    "# probs = [init]\n",
    "# p = csr_matrix(init)\n",
    "\n",
    "# damping = True\n",
    "# if damping:\n",
    "#     damping_factor = 0.85\n",
    "#     random_jump_vector = csr_matrix(np.ones(num_songs)/num_songs)\n",
    "#     for i in tqdm(range(30)):\n",
    "#         p = damping_factor * np.dot(p, transition_matrix) + (1 - damping_factor) * random_jump_vector\n",
    "#         probs.append(p)\n",
    "# else:\n",
    "#     for i in tqdm(range(30)):\n",
    "#         p = np.dot(p, transition_matrix)\n",
    "#         probs.append(p)\n",
    "\n",
    "\n",
    "# plot_data = []\n",
    "# for i in top_n_indices:\n",
    "#     song_name = song_data_map[list(song_relationships)[i]][\"song_name\"]\n",
    "#     for step_num, step in enumerate(probs):\n",
    "#         plot_data.append({'Iteration': step_num, 'Probability': step[0, i], 'Song': song_name})\n",
    "\n",
    "# df = pd.DataFrame(plot_data)\n",
    "# fig = px.line(df, x='Iteration', y='Probability', color='Song', title='Convergence of Most Popular Songs')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = transition_matrix.copy()\n",
    "# def user_playlist_vector(playlist_songs, song_indices, num_songs):\n",
    "#     user_vector = np.zeros(num_songs)\n",
    "#     for song in playlist_songs:\n",
    "#         user_vector[song_indices[song]] = 1/len(playlist_songs)\n",
    "#     return csr_matrix(user_vector)\n",
    "\n",
    "# def get_recommendation_vector(user_vector, transition_matrix, steps, damping = True, damping_factor = 0.0):\n",
    "#     p = user_vector\n",
    "#     if damping:\n",
    "#         random_jump_vector = csr_matrix(np.ones(num_songs) / num_songs)\n",
    "#         for _ in range(steps):\n",
    "#             p = damping_factor * np.dot(p, transition_matrix) + (1 - damping_factor) * user_vector\n",
    "#     else:\n",
    "#         for _ in range(steps):\n",
    "#             p = np.dot(p, transition_matrix)\n",
    "#     return p.toarray()[0]\n",
    "\n",
    "# def top_n_recommendations(recommendation_vector, song_data_map, n):\n",
    "#     top_indices = np.argsort(recommendation_vector)[-n:]\n",
    "#     top_songs = [song_data_map[list(song_relationships)[i]] for i in top_indices]\n",
    "#     return top_songs\n",
    "\n",
    "# def recommend_songs(user_playlist, song_indices, transition_matrix, song_data_map, steps, n=10):\n",
    "#     user_vector = user_playlist_vector(user_playlist, song_indices, len(song_indices))\n",
    "#     recommendation_vector = get_recommendation_vector(user_vector, transition_matrix, steps)\n",
    "#     return top_n_recommendations(recommendation_vector, song_data_map, n)\n",
    "\n",
    "# top_n_songs = [list(song_relationships)[i] for i in top_n_indices]\n",
    "# playlist = random.sample(list(song_relationships), 3) #+ top_n_songs\n",
    "\n",
    "# for i, j in zip(playlist, [song_data_map[x] for x in playlist]):\n",
    "#     print(i, j)\n",
    "\n",
    "# recommended_songs = recommend_songs(playlist, song_indices, A, song_data_map, 10, n=5)\n",
    "# for song in recommended_songs:\n",
    "#     print(f\"Song: {song['song_name']}\\nAlbum: {song['album_name']}\\nArtist: {song['artist_name']}\\n{'-'*40}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_occurrences = np.sum(cooccurrence_matrix)\n",
    "# p_i = np.sum(cooccurrence_matrix, axis=1) / total_occurrences\n",
    "# p_i = np.asarray(p_i).flatten()  # Convert matrix to 1D array\n",
    "# p_ij = cooccurrence_matrix / total_occurrences\n",
    "\n",
    "# pmi_data = np.array(p_ij.data)\n",
    "# k = 1\n",
    "\n",
    "# for i in tqdm(range(len(p_ij.indptr) - 1)):\n",
    "#     for data_idx in range(p_ij.indptr[i], p_ij.indptr[i + 1]):\n",
    "#         j = p_ij.indices[data_idx]\n",
    "#         if pmi_data[data_idx] > 0:  # Avoid log(0)\n",
    "#             original_pmi = np.log2(pmi_data[data_idx] / (p_i[i] * p_i[j]))\n",
    "#             pmi_data[data_idx] = original_pmi - (-(k - 1) * np.log2(pmi_data[data_idx]))\n",
    "#         # else:\n",
    "#         #     pmi_data[data_idx] = 0  # Or any other default value for log(0)\n",
    "\n",
    "# pmi_matrix = csr_matrix((pmi_data, p_ij.indices, p_ij.indptr), shape=cooccurrence_matrix.shape, dtype=np.float32)\n",
    "\n",
    "# save_path = os.path.join('song_data', 'pmi_matrix.gz')\n",
    "# with gzip.open(save_path, 'wb') as f:\n",
    "#     pickle.dump(pmi_matrix, f)\n",
    "# print(f\"'pmi_matrix' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pmi_chunk(start_idx, end_idx, p_i, p_ij, k=1):\n",
    "    pmi_data_chunk = []\n",
    "    row_indices_chunk = []\n",
    "    column_indices_chunk = []\n",
    "\n",
    "    for i in range(start_idx, end_idx):\n",
    "        for data_idx in range(p_ij.indptr[i], p_ij.indptr[i + 1]):\n",
    "            j = p_ij.indices[data_idx]\n",
    "\n",
    "            if i <= j:  # Only calculate PMI for upper triangle\n",
    "                value = p_ij.data[data_idx]\n",
    "                if value > 0:\n",
    "                    original_pmi = np.log2(value / (p_i[i] * p_i[j]))\n",
    "                    adjusted_pmi = original_pmi - (-(k - 1) * np.log2(value))\n",
    "\n",
    "                    # Store data for the upper triangle\n",
    "                    pmi_data_chunk.append(adjusted_pmi)\n",
    "                    row_indices_chunk.append(i)\n",
    "                    column_indices_chunk.append(j)\n",
    "\n",
    "                    if i != j:\n",
    "                        # Store symmetric data for the lower triangle\n",
    "                        pmi_data_chunk.append(adjusted_pmi)\n",
    "                        row_indices_chunk.append(j)\n",
    "                        column_indices_chunk.append(i)\n",
    "\n",
    "    return csr_matrix((pmi_data_chunk, (row_indices_chunk, column_indices_chunk)), shape=p_ij.shape, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Breaking up p_ij matrix rows into chunks\n",
    "chunk_size = 5000  # Adjust based on your memory constraints\n",
    "num_chunks = len(p_ij.indptr) // chunk_size + 1\n",
    "interim_matrices = []\n",
    "\n",
    "for chunk in range(num_chunks):\n",
    "    start_idx = chunk * chunk_size\n",
    "    end_idx = min((chunk + 1) * chunk_size, len(p_ij.indptr) - 1)\n",
    "    \n",
    "    interim_matrix = process_pmi_chunk(start_idx, end_idx, p_i, p_ij)\n",
    "    interim_path = os.path.join('song_data', f'interim_pmi_matrix_{chunk}.gz')\n",
    "    with gzip.open(interim_path, 'wb') as f:\n",
    "        pickle.dump(interim_matrix, f)\n",
    "    interim_matrices.append(interim_path)\n",
    "    print(f\"Processed PMI chunk {chunk + 1}/{num_chunks} and saved to {interim_path}\")\n",
    "\n",
    "# Combining interim results to get the final PMI matrix\n",
    "pmi_matrix = vstack([pickle.load(gzip.open(path, 'rb')) for path in interim_matrices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load pmi_matrix from storage\n",
    "\n",
    "save_path = os.path.join('song_data', 'pmi_matrix.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    pmi_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmi_data = np.array(p_ij.data)\n",
    "\n",
    "# for i in tqdm(range(len(p_ij.indptr) - 1)):\n",
    "#     for data_idx in range(p_ij.indptr[i], p_ij.indptr[i + 1]):\n",
    "#         j = p_ij.indices[data_idx]\n",
    "#         if pmi_data[data_idx] > 0:  # Avoid log(0)\n",
    "#             pmi_data[data_idx] = np.log2(pmi_data[data_idx] / (p_i[i] * p_i[j]))\n",
    "#         else:\n",
    "#             pmi_data[data_idx] = 0  # Or any other default value for log(0)\n",
    "\n",
    "# pmi_matrix = csr_matrix((pmi_data, p_ij.indices, p_ij.indptr), shape=cooccurence_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_playlist_vector(playlist_songs, song_indices, num_songs):\n",
    "    user_vector = np.zeros(num_songs)\n",
    "    for song in playlist_songs:\n",
    "        user_vector[song_indices[song]] = 1\n",
    "    return csr_matrix(user_vector)\n",
    "\n",
    "def compute_scores(user_vector, pmi_matrix):\n",
    "    scores = user_vector.dot(pmi_matrix)\n",
    "    return scores.toarray()[0]\n",
    "\n",
    "def get_top_recommendations(scores, song_data_map, n=10):\n",
    "    top_indices = np.argsort(scores)[-n:]\n",
    "    top_songs = [song_data_map[list(song_relationships)[i]] for i in top_indices]\n",
    "    return top_songs\n",
    "\n",
    "def recommend_songs_pmi(user_playlist, song_indices, pmi_matrix, song_data_map, n=10):\n",
    "    user_vector = user_playlist_vector(user_playlist, song_indices, num_songs)\n",
    "    user_vector_csr = csr_matrix(user_vector)\n",
    "    scores = compute_scores(user_vector_csr, pmi_matrix)\n",
    "    return get_top_recommendations(scores, song_data_map, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongRecommendationApp(tk.Tk):\n",
    "    def __init__(self, song_data_map):\n",
    "        super().__init__()\n",
    "\n",
    "        default_font = font.nametofont(\"TkDefaultFont\")\n",
    "        default_font.configure(family=\"Courier\")\n",
    "\n",
    "        self.song_data_map = song_data_map\n",
    "        self.uri_map = {self.format_song_display(song_info): uri for uri, song_info in song_data_map.items()}\n",
    "        self.playlist_data = []  # Store song data for sorting\n",
    "\n",
    "        # Filter Frame\n",
    "        self.filter_frame = ttk.Frame(self)\n",
    "        self.filter_frame.pack(pady=10)\n",
    "\n",
    "        # Label and Entry for Song\n",
    "        self.song_label = ttk.Label(self.filter_frame, text=\"Song\")\n",
    "        self.song_label.grid(row=0, column=0, padx=5)\n",
    "        self.song_entry = ttk.Entry(self.filter_frame)\n",
    "        self.song_entry.grid(row=1, column=0, padx=5)\n",
    "\n",
    "        # Label and Entry for Artist\n",
    "        self.artist_label = ttk.Label(self.filter_frame, text=\"Artist\")\n",
    "        self.artist_label.grid(row=0, column=1, padx=5)\n",
    "        self.artist_entry = ttk.Entry(self.filter_frame)\n",
    "        self.artist_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "        # Label and Entry for Album\n",
    "        self.album_label = ttk.Label(self.filter_frame, text=\"Album\")\n",
    "        self.album_label.grid(row=0, column=2, padx=5)\n",
    "        self.album_entry = ttk.Entry(self.filter_frame)\n",
    "        self.album_entry.grid(row=1, column=2, padx=5)\n",
    "\n",
    "        # Debounce logic\n",
    "        self.last_time = time.time()\n",
    "\n",
    "        self.search_button = ttk.Button(self.filter_frame, text=\"Search\", command=self.display_search_results)\n",
    "        self.search_button.grid(row=2, columnspan=3, pady=10)\n",
    "\n",
    "        width = 200\n",
    "        # Songs Listbox\n",
    "        self.songs_listbox = tk.Listbox(self, selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.songs_listbox.pack(pady=10)\n",
    "\n",
    "        # Drag & Drop functionality\n",
    "        self.songs_listbox.bind('<<ListboxSelect>>', self.add_to_playlist)\n",
    "\n",
    "        # Playlist Listbox\n",
    "        self.playlist_listbox = tk.Listbox(self, bg=\"lightblue\", selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.playlist_listbox.pack(pady=10)\n",
    "\n",
    "        # Number of recommendations\n",
    "        self.n_label = ttk.Label(self, text=\"Number of Recommendations:\")\n",
    "        self.n_label.pack(pady=5)\n",
    "        self.n_entry = ttk.Entry(self)\n",
    "        self.n_entry.pack(pady=5)\n",
    "\n",
    "        # Button to generate recommendations\n",
    "        self.btn_recommend = ttk.Button(self, text=\"Generate Recommendations\", command=self.generate_recommendations)\n",
    "        self.btn_recommend.pack(pady=10)\n",
    "\n",
    "        # Recommendations Listbox\n",
    "        self.recommendations_listbox = tk.Listbox(self, bg=\"lightgreen\", selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.recommendations_listbox.pack(pady=10)\n",
    "\n",
    "        self.recommendations_listbox.bind('<Double-Button-1>', self.open_in_spotify)\n",
    "\n",
    "        self.btn_refresh = ttk.Button(self, text=\"Refresh\", command=self.refresh)\n",
    "        self.btn_refresh.pack(pady=10)\n",
    "    \n",
    "    def refresh(self):\n",
    "        # Clear all fields\n",
    "        self.song_entry.delete(0, tk.END)\n",
    "        self.artist_entry.delete(0, tk.END)\n",
    "        self.album_entry.delete(0, tk.END)\n",
    "        self.n_entry.delete(0, tk.END)\n",
    "        \n",
    "        # Clear listboxes\n",
    "        self.songs_listbox.delete(0, tk.END)\n",
    "        self.playlist_listbox.delete(0, tk.END)\n",
    "        self.recommendations_listbox.delete(0, tk.END)\n",
    "\n",
    "    def open_in_spotify(self, event):\n",
    "            selected_index = self.recommendations_listbox.curselection()\n",
    "            if selected_index:\n",
    "                selected_song = self.recommendations_listbox.get(selected_index)\n",
    "                song_uri = self.uri_map[selected_song]\n",
    "                uri = song_uri.split(':')[-1]\n",
    "                webbrowser.open(f\"https://open.spotify.com/track/{uri}\")\n",
    "                \n",
    "    def display_search_results(self):\n",
    "        song_query = self.song_entry.get().lower()\n",
    "        artist_query = self.artist_entry.get().lower()\n",
    "        album_query = self.album_entry.get().lower()\n",
    "\n",
    "        self.songs_listbox.delete(0, tk.END)\n",
    "        results = []  # Store the filtered results first\n",
    "\n",
    "        for uri, song_info in self.song_data_map.items():\n",
    "            if song_query in song_info['song_name'].lower() and artist_query in song_info['artist_name'].lower() and album_query in song_info['album_name'].lower():\n",
    "                display_name = self.format_song_display(song_info)\n",
    "                results.append(display_name)\n",
    "\n",
    "        # Sort by album name\n",
    "        results.sort(key=lambda x: self.song_data_map[self.uri_map[x]]['album_name'])\n",
    "\n",
    "        # Display the sorted results\n",
    "        for display_name in results:\n",
    "            self.songs_listbox.insert(tk.END, display_name)\n",
    "\n",
    "        if len(results) > 200:  # If you want to limit the displayed results\n",
    "            self.songs_listbox.delete(201, tk.END)\n",
    "\n",
    "    def format_song_display(self, song_info):\n",
    "        formatted_str = \"{:<65}{:<35}{:<35}\"\n",
    "        f_string = formatted_str.format(song_info['song_name'], song_info['artist_name'], song_info['album_name'])\n",
    "        return f_string\n",
    "\n",
    "    def add_to_playlist(self, event):\n",
    "        selected_index = self.songs_listbox.curselection()\n",
    "        if selected_index:  # This checks if there's any selection at all\n",
    "            selected_song = self.songs_listbox.get(selected_index)\n",
    "            if selected_song not in self.playlist_listbox.get(0, tk.END):  # Prevent duplicates\n",
    "                self.playlist_listbox.insert(tk.END, selected_song)\n",
    "\n",
    "    def generate_recommendations(self):\n",
    "        playlist_display_names = list(self.playlist_listbox.get(0, tk.END))\n",
    "        playlist_uris = [self.uri_map[display_name] for display_name in playlist_display_names]  # Extract URIs\n",
    "\n",
    "        n = int(self.n_entry.get())\n",
    "        recommended_songs = recommend_songs_pmi(playlist_uris, song_indices, pmi_matrix, song_data_map, n)\n",
    "\n",
    "        self.recommendations_listbox.delete(0, tk.END)\n",
    "        for song in recommended_songs:\n",
    "            formatted_song = self.format_song_display(song)\n",
    "            self.recommendations_listbox.insert(tk.END, formatted_song)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = SongRecommendationApp(song_data_map)\n",
    "    app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = random.sample(list(song_relationships), 1)\n",
    "\n",
    "for i, j in zip(playlist, [song_data_map[x] for x in playlist]):\n",
    "    print(f\"Song: {j['song_name']}\\nAlbum: {j['album_name']}\\nArtist: {j['artist_name']}\\n{'-'*40}\")\n",
    "\n",
    "print(f'\\n{\"-\"*60}')\n",
    "recommended_songs = recommend_songs_pmi(playlist, song_indices, pmi_matrix, song_data_map, n=8)\n",
    "for song in recommended_songs:\n",
    "    print(f\"Song: {song['song_name']}\\nAlbum: {song['album_name']}\\nArtist: {song['artist_name']}\\n{'-'*40}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
