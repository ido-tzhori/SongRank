{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from scipy.sparse import csr_matrix, vstack, csr_array\n",
    "import time\n",
    "import random\n",
    "import webbrowser\n",
    "import pickle, gzip, joblib, shelve\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, font\n",
    "import threading, time\n",
    "from itertools import islice, combinations\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 playlists\n"
     ]
    }
   ],
   "source": [
    "directory_path = 'data/raw'\n",
    "filenames = sorted(os.listdir(directory_path))\n",
    "print(f\"{len(filenames) * 1000} playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# looking at only the first 30,000 playlists\n",
    "fullpaths = [directory_path + '/' + f for f in filenames][0:985]\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "song_relationships = {}\n",
    "iteration_times = []\n",
    "\n",
    "song_uris_set = set()\n",
    "\n",
    "for idx, path in enumerate(fullpaths):\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(path) as f:\n",
    "        mpd_slice = json.load(f)\n",
    "\n",
    "    playlists_data = mpd_slice['playlists']\n",
    "\n",
    "    for playlist in playlists_data:\n",
    "        songs_set = set()\n",
    "\n",
    "        for track in playlist['tracks']:\n",
    "            song_uri = track['track_uri'].split(':')[-1]            \n",
    "            songs_set.add(song_uri)\n",
    "\n",
    "        # Compute song-to-song relationships for the current playlist\n",
    "        pair_counts = Counter(combinations(songs_set, 2))\n",
    "        \n",
    "        for (song1, song2), count in pair_counts.items():\n",
    "            song_relationships.setdefault(song1, {}).setdefault(song2, 0)\n",
    "            song_relationships[song1][song2] += count\n",
    "            song_relationships.setdefault(song2, {}).setdefault(song1, 0)\n",
    "            song_relationships[song2][song1] += count\n",
    "\n",
    "    end_time = time.time()\n",
    "    iteration_time = end_time - start_time\n",
    "    iteration_times.append(iteration_time)\n",
    "\n",
    "    if idx % 10 == 0 and idx > 0:\n",
    "        t = np.sum(iteration_times[-10:])\n",
    "        print(f\"processing {idx - 10}-{idx} - time taken {t:.2f}\")\n",
    "\n",
    "print(f'{len(song_relationships)} songs processed')\n",
    "\n",
    "if save:\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "\n",
    "    save_path = os.path.join('song_data', f'{formatted_time}_song_relationships.gz')\n",
    "\n",
    "    with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(song_relationships, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"'song_relationships' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to load song_relationships from storage\n",
    "\n",
    "save_path = os.path.join('song_data', '18_14_08_2023_song_relationships.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    song_relationships = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "song_data_map = {}\n",
    "for idx, path in enumerate(fullpaths):\n",
    "    if idx % 100 == 0 and idx > 0:\n",
    "        print(f\"Processed {idx-100}-{idx}\")\n",
    "    with open(path) as f:\n",
    "        mpd_slice = json.load(f)\n",
    "    playlists_data = mpd_slice['playlists']\n",
    "    for playlist in playlists_data:\n",
    "        for track in playlist['tracks']:\n",
    "            song_uri = track['track_uri'].split(':')[-1]\n",
    "            song_name = track['track_name']\n",
    "            album_name = track['album_name']\n",
    "            artist_name = track['artist_name']\n",
    "            artist_uri = track['artist_uri']\n",
    "            album_uri = track['album_uri']\n",
    "            if song_uri in song_relationships:\n",
    "                song_data_map[song_uri] = {'song_name': song_name, 'album_name': album_name, 'artist_name': artist_name,\n",
    "                                          'artist_uri': artist_uri, 'album_uri': album_uri}\n",
    "                \n",
    "print(f'{len(song_data_map)} songs processed')\n",
    "\n",
    "if save:\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "    save_path = os.path.join('song_data', f'{formatted_time}_song_data_map.gz')\n",
    "\n",
    "    with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(song_data_map, f)\n",
    "    print(f\"'song_data_map' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244869\n"
     ]
    }
   ],
   "source": [
    "# to load song_data_map from storage\n",
    "save_path = os.path.join('song_data', f'20_14_08_2023_song_data_map.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    song_data_map = pickle.load(f)\n",
    "    \n",
    "print(len(song_data_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "song_indices = {song_uri: idx for idx, song_uri in enumerate(song_relationships.keys())}\n",
    "\n",
    "if save:\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "    save_path = os.path.join('song_data', f'{formatted_time}_song_indices.gz')\n",
    "    with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(song_indices, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"'song_indices' saved to {save_path}\")\n",
    "\n",
    "num_songs = len(song_indices)\n",
    "\n",
    "print(num_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244869\n"
     ]
    }
   ],
   "source": [
    "# to load song_indices from storage\n",
    "save_path = os.path.join('song_data', f'20_14_08_2023_song_indices.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    song_indices = pickle.load(f)\n",
    "\n",
    "num_songs = len(song_indices)\n",
    "print(num_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song with the most connections: {'song_name': 'Closer', 'album_name': 'Closer', 'artist_name': 'The Chainsmokers', 'artist_uri': 'spotify:artist:69GGBxA162lTqCwzJG5jLp', 'album_uri': 'spotify:album:0rSLgV8p5FzfnqlEk4GzxE'}\n",
      "Number of connections: 203549\n"
     ]
    }
   ],
   "source": [
    "max_connections = 0\n",
    "song_with_most_connections = None\n",
    "\n",
    "for song, connections in song_relationships.items():\n",
    "    num_connections = len(connections)\n",
    "    if num_connections > max_connections:\n",
    "        max_connections = num_connections\n",
    "        song_with_most_connections = song\n",
    "\n",
    "print(\"Song with the most connections:\", song_data_map[song_with_most_connections])\n",
    "print(\"Number of connections:\", max_connections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed chunk 1/225\n",
      "processed chunk 2/225\n",
      "processed chunk 3/225\n",
      "processed chunk 4/225\n",
      "processed chunk 5/225\n",
      "processed chunk 6/225\n",
      "processed chunk 7/225\n",
      "processed chunk 8/225\n",
      "processed chunk 9/225\n",
      "processed chunk 10/225\n",
      "processed chunk 11/225\n",
      "processed chunk 12/225\n",
      "processed chunk 13/225\n",
      "processed chunk 14/225\n",
      "processed chunk 15/225\n",
      "processed chunk 16/225\n",
      "processed chunk 17/225\n",
      "processed chunk 18/225\n",
      "processed chunk 19/225\n",
      "processed chunk 20/225\n",
      "processed chunk 21/225\n",
      "processed chunk 22/225\n",
      "processed chunk 23/225\n",
      "processed chunk 24/225\n",
      "processed chunk 25/225\n",
      "processed chunk 26/225\n",
      "processed chunk 27/225\n",
      "processed chunk 28/225\n",
      "processed chunk 29/225\n",
      "processed chunk 30/225\n",
      "processed chunk 31/225\n",
      "processed chunk 32/225\n",
      "processed chunk 33/225\n",
      "processed chunk 34/225\n",
      "processed chunk 35/225\n",
      "processed chunk 36/225\n",
      "processed chunk 37/225\n",
      "processed chunk 38/225\n",
      "processed chunk 39/225\n",
      "processed chunk 40/225\n",
      "processed chunk 41/225\n",
      "processed chunk 42/225\n",
      "processed chunk 43/225\n",
      "processed chunk 44/225\n",
      "processed chunk 45/225\n",
      "processed chunk 46/225\n",
      "processed chunk 47/225\n",
      "processed chunk 48/225\n",
      "processed chunk 49/225\n",
      "processed chunk 50/225\n",
      "processed chunk 51/225\n",
      "processed chunk 52/225\n",
      "processed chunk 53/225\n",
      "processed chunk 54/225\n",
      "processed chunk 55/225\n",
      "processed chunk 56/225\n",
      "processed chunk 57/225\n",
      "processed chunk 58/225\n",
      "processed chunk 59/225\n",
      "processed chunk 60/225\n",
      "processed chunk 61/225\n",
      "processed chunk 62/225\n",
      "processed chunk 63/225\n",
      "processed chunk 64/225\n",
      "processed chunk 65/225\n",
      "processed chunk 66/225\n",
      "processed chunk 67/225\n",
      "processed chunk 68/225\n",
      "processed chunk 69/225\n",
      "processed chunk 70/225\n",
      "processed chunk 71/225\n",
      "processed chunk 72/225\n",
      "processed chunk 73/225\n",
      "processed chunk 74/225\n",
      "processed chunk 75/225\n",
      "processed chunk 76/225\n",
      "processed chunk 77/225\n",
      "processed chunk 78/225\n",
      "processed chunk 79/225\n",
      "processed chunk 80/225\n",
      "processed chunk 81/225\n",
      "processed chunk 82/225\n",
      "processed chunk 83/225\n",
      "processed chunk 84/225\n",
      "processed chunk 85/225\n",
      "processed chunk 86/225\n",
      "processed chunk 87/225\n",
      "processed chunk 88/225\n",
      "processed chunk 89/225\n",
      "processed chunk 90/225\n",
      "processed chunk 91/225\n",
      "processed chunk 92/225\n",
      "processed chunk 93/225\n",
      "processed chunk 94/225\n",
      "processed chunk 95/225\n",
      "processed chunk 96/225\n",
      "processed chunk 97/225\n",
      "processed chunk 98/225\n",
      "processed chunk 99/225\n",
      "processed chunk 100/225\n",
      "processed chunk 101/225\n",
      "processed chunk 102/225\n",
      "processed chunk 103/225\n",
      "processed chunk 104/225\n",
      "processed chunk 105/225\n",
      "processed chunk 106/225\n",
      "processed chunk 107/225\n",
      "processed chunk 108/225\n",
      "processed chunk 109/225\n",
      "processed chunk 110/225\n",
      "processed chunk 111/225\n",
      "processed chunk 112/225\n",
      "processed chunk 113/225\n",
      "processed chunk 114/225\n",
      "processed chunk 115/225\n",
      "processed chunk 116/225\n",
      "processed chunk 117/225\n",
      "processed chunk 118/225\n",
      "processed chunk 119/225\n",
      "processed chunk 120/225\n",
      "processed chunk 121/225\n",
      "processed chunk 122/225\n",
      "processed chunk 123/225\n",
      "processed chunk 124/225\n",
      "processed chunk 125/225\n",
      "processed chunk 126/225\n",
      "processed chunk 127/225\n",
      "processed chunk 128/225\n",
      "processed chunk 129/225\n",
      "processed chunk 130/225\n",
      "processed chunk 131/225\n",
      "processed chunk 132/225\n",
      "processed chunk 133/225\n",
      "processed chunk 134/225\n",
      "processed chunk 135/225\n",
      "processed chunk 136/225\n",
      "processed chunk 137/225\n",
      "processed chunk 138/225\n",
      "processed chunk 139/225\n",
      "processed chunk 140/225\n",
      "processed chunk 141/225\n",
      "processed chunk 142/225\n",
      "processed chunk 143/225\n",
      "processed chunk 144/225\n",
      "processed chunk 145/225\n",
      "processed chunk 146/225\n",
      "processed chunk 147/225\n",
      "processed chunk 148/225\n",
      "processed chunk 149/225\n",
      "processed chunk 150/225\n",
      "processed chunk 151/225\n",
      "processed chunk 152/225\n",
      "processed chunk 153/225\n",
      "processed chunk 154/225\n",
      "processed chunk 155/225\n",
      "processed chunk 156/225\n",
      "processed chunk 157/225\n",
      "processed chunk 158/225\n",
      "processed chunk 159/225\n",
      "processed chunk 160/225\n",
      "processed chunk 161/225\n",
      "processed chunk 162/225\n",
      "processed chunk 163/225\n",
      "processed chunk 164/225\n",
      "processed chunk 165/225\n",
      "processed chunk 166/225\n",
      "processed chunk 167/225\n",
      "processed chunk 168/225\n",
      "processed chunk 169/225\n",
      "processed chunk 170/225\n",
      "processed chunk 171/225\n",
      "processed chunk 172/225\n",
      "processed chunk 173/225\n",
      "processed chunk 174/225\n",
      "processed chunk 175/225\n",
      "processed chunk 176/225\n",
      "processed chunk 177/225\n",
      "processed chunk 178/225\n",
      "processed chunk 179/225\n",
      "processed chunk 180/225\n",
      "processed chunk 181/225\n",
      "processed chunk 182/225\n",
      "processed chunk 183/225\n",
      "processed chunk 184/225\n",
      "processed chunk 185/225\n",
      "processed chunk 186/225\n",
      "processed chunk 187/225\n",
      "processed chunk 188/225\n",
      "processed chunk 189/225\n",
      "processed chunk 190/225\n",
      "processed chunk 191/225\n",
      "processed chunk 192/225\n",
      "processed chunk 193/225\n",
      "processed chunk 194/225\n",
      "processed chunk 195/225\n",
      "processed chunk 196/225\n",
      "processed chunk 197/225\n",
      "processed chunk 198/225\n",
      "processed chunk 199/225\n",
      "processed chunk 200/225\n",
      "processed chunk 201/225\n",
      "processed chunk 202/225\n",
      "processed chunk 203/225\n",
      "processed chunk 204/225\n",
      "processed chunk 205/225\n",
      "processed chunk 206/225\n",
      "processed chunk 207/225\n",
      "processed chunk 208/225\n",
      "processed chunk 209/225\n",
      "processed chunk 210/225\n",
      "processed chunk 211/225\n",
      "processed chunk 212/225\n",
      "processed chunk 213/225\n",
      "processed chunk 214/225\n",
      "processed chunk 215/225\n",
      "processed chunk 216/225\n",
      "processed chunk 217/225\n",
      "processed chunk 218/225\n",
      "processed chunk 219/225\n",
      "processed chunk 220/225\n",
      "processed chunk 221/225\n",
      "processed chunk 222/225\n",
      "processed chunk 223/225\n",
      "processed chunk 224/225\n",
      "processed chunk 225/225\n",
      "finished processing matrix size: (2244869, 2244869)\n",
      "'cooccurrence_matrix' saved to song_data/02_15_08_2023_cooccurrence_matrix.gz\n"
     ]
    }
   ],
   "source": [
    "def slice_dict(d, start, end):\n",
    "    return dict(islice(d.items(), start, end))\n",
    "\n",
    "def update_matrix(matrix, chunk):\n",
    "    data = []\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "\n",
    "    for song_uri, relationships in chunk.items():\n",
    "        row_idx = song_indices[song_uri]\n",
    "        for related_song_uri, count in relationships.items():\n",
    "            col_idx = song_indices[related_song_uri]\n",
    "            \n",
    "            if row_idx <= col_idx:\n",
    "                data.append(count)\n",
    "                row_indices.append(row_idx)\n",
    "                col_indices.append(col_idx)\n",
    "                \n",
    "                if row_idx != col_idx:\n",
    "                    data.append(count)\n",
    "                    row_indices.append(col_idx)\n",
    "                    col_indices.append(row_idx)\n",
    "\n",
    "    # Create a temporary csr_matrix\n",
    "    temp_matrix = csr_matrix((data, (row_indices, col_indices)), shape=(num_songs, num_songs), dtype=np.int32)\n",
    "    \n",
    "    return matrix + temp_matrix\n",
    "\n",
    "cooccurrence_matrix = csr_matrix((num_songs, num_songs), dtype=np.int32)\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "num_chunks = (num_songs + chunk_size - 1) // chunk_size\n",
    "\n",
    "for start_index in range(0, num_songs, chunk_size):\n",
    "    end_index = min(start_index + chunk_size, num_songs)\n",
    "    current_relationships = slice_dict(song_relationships, start_index, end_index)\n",
    "    cooccurrence_matrix = update_matrix(cooccurrence_matrix, current_relationships)\n",
    "    print(f\"processed chunk {start_index//chunk_size + 1}/{num_chunks}\")\n",
    "\n",
    "# Now, the full_matrix is your final cooccurrence_matrix\n",
    "\n",
    "print(f'finished processing matrix size: {cooccurrence_matrix.shape}')\n",
    "\n",
    "if save:\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "    save_path = os.path.join('song_data', f'{formatted_time}_cooccurrence_matrix.gz')\n",
    "    with gzip.open(save_path, 'wb') as f:\n",
    "        pickle.dump(cooccurrence_matrix, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"'cooccurrence_matrix' saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity(csr_matrix):\n",
    "    total_elements = csr_matrix.shape[0] * csr_matrix.shape[1]\n",
    "    non_zero_elements = csr_matrix.nnz\n",
    "    sparsity = (total_elements - non_zero_elements) / total_elements\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2244869, 2244869)\n"
     ]
    }
   ],
   "source": [
    "# to load cooccurrence_matrix from storage\n",
    "save_path = os.path.join('song_data', f'02_15_08_2023_cooccurrence_matrix.gz')\n",
    "\n",
    "with gzip.open(save_path, 'rb') as f:\n",
    "    cooccurrence_matrix = pickle.load(f)\n",
    "    \n",
    "print(cooccurrence_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rows 0 to 5000\n",
      "Processing rows 5000 to 10000\n",
      "Processing rows 10000 to 15000\n",
      "Processing rows 15000 to 20000\n",
      "Processing rows 20000 to 25000\n",
      "Processing rows 25000 to 30000\n",
      "Processing rows 30000 to 35000\n",
      "Processing rows 35000 to 40000\n",
      "Processing rows 40000 to 45000\n",
      "Processing rows 45000 to 50000\n",
      "Processing rows 50000 to 55000\n",
      "Processing rows 55000 to 60000\n",
      "Processing rows 60000 to 65000\n",
      "Processing rows 65000 to 70000\n",
      "Processing rows 70000 to 75000\n",
      "Processing rows 75000 to 80000\n",
      "Processing rows 80000 to 85000\n",
      "Processing rows 85000 to 90000\n",
      "Processing rows 90000 to 95000\n",
      "Processing rows 95000 to 100000\n",
      "Processing rows 100000 to 105000\n",
      "Processing rows 105000 to 110000\n",
      "Processing rows 110000 to 115000\n",
      "Processing rows 115000 to 120000\n",
      "Processing rows 120000 to 125000\n",
      "Processing rows 125000 to 130000\n",
      "Processing rows 130000 to 135000\n",
      "Processing rows 135000 to 140000\n",
      "Processing rows 140000 to 145000\n",
      "Processing rows 145000 to 150000\n",
      "Processing rows 150000 to 155000\n",
      "Processing rows 155000 to 160000\n",
      "Processing rows 160000 to 165000\n",
      "Processing rows 165000 to 170000\n",
      "Processing rows 170000 to 175000\n",
      "Processing rows 175000 to 180000\n",
      "Processing rows 180000 to 185000\n",
      "Processing rows 185000 to 190000\n",
      "Processing rows 190000 to 195000\n",
      "Processing rows 195000 to 200000\n",
      "Processing rows 200000 to 205000\n",
      "Processing rows 205000 to 210000\n",
      "Processing rows 210000 to 215000\n",
      "Processing rows 215000 to 220000\n",
      "Processing rows 220000 to 225000\n",
      "Processing rows 225000 to 230000\n",
      "Processing rows 230000 to 235000\n",
      "Processing rows 235000 to 240000\n",
      "Processing rows 240000 to 245000\n",
      "Processing rows 245000 to 250000\n",
      "Processing rows 250000 to 255000\n",
      "Processing rows 255000 to 260000\n",
      "Processing rows 260000 to 265000\n",
      "Processing rows 265000 to 270000\n",
      "Processing rows 270000 to 275000\n",
      "Processing rows 275000 to 280000\n",
      "Processing rows 280000 to 285000\n",
      "Processing rows 285000 to 290000\n",
      "Processing rows 290000 to 295000\n",
      "Processing rows 295000 to 300000\n",
      "Processing rows 300000 to 305000\n",
      "Processing rows 305000 to 310000\n",
      "Processing rows 310000 to 315000\n",
      "Processing rows 315000 to 320000\n",
      "Processing rows 320000 to 325000\n",
      "Processing rows 325000 to 330000\n",
      "Processing rows 330000 to 335000\n",
      "Processing rows 335000 to 340000\n",
      "Processing rows 340000 to 345000\n",
      "Processing rows 345000 to 350000\n",
      "Processing rows 350000 to 355000\n",
      "Processing rows 355000 to 360000\n",
      "Processing rows 360000 to 365000\n",
      "Processing rows 365000 to 370000\n",
      "Processing rows 370000 to 375000\n",
      "Processing rows 375000 to 380000\n",
      "Processing rows 380000 to 385000\n",
      "Processing rows 385000 to 390000\n",
      "Processing rows 390000 to 395000\n",
      "Processing rows 395000 to 400000\n",
      "Processing rows 400000 to 405000\n",
      "Processing rows 405000 to 410000\n",
      "Processing rows 410000 to 415000\n",
      "Processing rows 415000 to 420000\n",
      "Processing rows 420000 to 425000\n",
      "Processing rows 425000 to 430000\n",
      "Processing rows 430000 to 435000\n",
      "Processing rows 435000 to 440000\n",
      "Processing rows 440000 to 445000\n",
      "Processing rows 445000 to 450000\n",
      "Processing rows 450000 to 455000\n",
      "Processing rows 455000 to 460000\n",
      "Processing rows 460000 to 465000\n",
      "Processing rows 465000 to 470000\n",
      "Processing rows 470000 to 475000\n",
      "Processing rows 475000 to 480000\n",
      "Processing rows 480000 to 485000\n",
      "Processing rows 485000 to 490000\n",
      "Processing rows 490000 to 495000\n",
      "Processing rows 495000 to 500000\n",
      "Processing rows 500000 to 505000\n",
      "Processing rows 505000 to 510000\n",
      "Processing rows 510000 to 515000\n",
      "Processing rows 515000 to 520000\n",
      "Processing rows 520000 to 525000\n",
      "Processing rows 525000 to 530000\n",
      "Processing rows 530000 to 535000\n",
      "Processing rows 535000 to 540000\n",
      "Processing rows 540000 to 545000\n",
      "Processing rows 545000 to 550000\n",
      "Processing rows 550000 to 555000\n",
      "Processing rows 555000 to 560000\n",
      "Processing rows 560000 to 565000\n",
      "Processing rows 565000 to 570000\n",
      "Processing rows 570000 to 575000\n",
      "Processing rows 575000 to 580000\n",
      "Processing rows 580000 to 585000\n",
      "Processing rows 585000 to 590000\n",
      "Processing rows 590000 to 595000\n",
      "Processing rows 595000 to 600000\n",
      "Processing rows 600000 to 605000\n",
      "Processing rows 605000 to 610000\n",
      "Processing rows 610000 to 615000\n",
      "Processing rows 615000 to 620000\n",
      "Processing rows 620000 to 625000\n",
      "Processing rows 625000 to 630000\n",
      "Processing rows 630000 to 635000\n",
      "Processing rows 635000 to 640000\n",
      "Processing rows 640000 to 645000\n",
      "Processing rows 645000 to 650000\n",
      "Processing rows 650000 to 655000\n",
      "Processing rows 655000 to 660000\n",
      "Processing rows 660000 to 665000\n",
      "Processing rows 665000 to 670000\n",
      "Processing rows 670000 to 675000\n",
      "Processing rows 675000 to 680000\n",
      "Processing rows 680000 to 685000\n",
      "Processing rows 685000 to 690000\n",
      "Processing rows 690000 to 695000\n",
      "Processing rows 695000 to 700000\n",
      "Processing rows 700000 to 705000\n",
      "Processing rows 705000 to 710000\n",
      "Processing rows 710000 to 715000\n",
      "Processing rows 715000 to 720000\n",
      "Processing rows 720000 to 725000\n",
      "Processing rows 725000 to 730000\n",
      "Processing rows 730000 to 735000\n",
      "Processing rows 735000 to 740000\n",
      "Processing rows 740000 to 745000\n",
      "Processing rows 745000 to 750000\n",
      "Processing rows 750000 to 755000\n",
      "Processing rows 755000 to 760000\n",
      "Processing rows 760000 to 765000\n",
      "Processing rows 765000 to 770000\n",
      "Processing rows 770000 to 775000\n",
      "Processing rows 775000 to 780000\n",
      "Processing rows 780000 to 785000\n",
      "Processing rows 785000 to 790000\n",
      "Processing rows 790000 to 795000\n",
      "Processing rows 795000 to 800000\n",
      "Processing rows 800000 to 805000\n",
      "Processing rows 805000 to 810000\n",
      "Processing rows 810000 to 815000\n",
      "Processing rows 815000 to 820000\n",
      "Processing rows 820000 to 825000\n",
      "Processing rows 825000 to 830000\n",
      "Processing rows 830000 to 835000\n",
      "Processing rows 835000 to 840000\n",
      "Processing rows 840000 to 845000\n",
      "Processing rows 845000 to 850000\n",
      "Processing rows 850000 to 855000\n",
      "Processing rows 855000 to 860000\n",
      "Processing rows 860000 to 865000\n",
      "Processing rows 865000 to 870000\n",
      "Processing rows 870000 to 875000\n",
      "Processing rows 875000 to 880000\n",
      "Processing rows 880000 to 885000\n",
      "Processing rows 885000 to 890000\n",
      "Processing rows 890000 to 895000\n",
      "Processing rows 895000 to 900000\n",
      "Processing rows 900000 to 905000\n",
      "Processing rows 905000 to 910000\n",
      "Processing rows 910000 to 915000\n",
      "Processing rows 915000 to 920000\n",
      "Processing rows 920000 to 925000\n",
      "Processing rows 925000 to 930000\n",
      "Processing rows 930000 to 935000\n",
      "Processing rows 935000 to 940000\n",
      "Processing rows 940000 to 945000\n",
      "Processing rows 945000 to 950000\n",
      "Processing rows 950000 to 955000\n",
      "Processing rows 955000 to 960000\n",
      "Processing rows 960000 to 965000\n",
      "Processing rows 965000 to 970000\n",
      "Processing rows 970000 to 975000\n",
      "Processing rows 975000 to 980000\n",
      "Processing rows 980000 to 985000\n",
      "Processing rows 985000 to 990000\n",
      "Processing rows 990000 to 995000\n",
      "Processing rows 995000 to 1000000\n",
      "Processing rows 1000000 to 1005000\n",
      "Processing rows 1005000 to 1010000\n",
      "Processing rows 1010000 to 1015000\n",
      "Processing rows 1015000 to 1020000\n",
      "Processing rows 1020000 to 1025000\n",
      "Processing rows 1025000 to 1030000\n",
      "Processing rows 1030000 to 1035000\n",
      "Processing rows 1035000 to 1040000\n",
      "Processing rows 1040000 to 1045000\n",
      "Processing rows 1045000 to 1050000\n",
      "Processing rows 1050000 to 1055000\n",
      "Processing rows 1055000 to 1060000\n",
      "Processing rows 1060000 to 1065000\n",
      "Processing rows 1065000 to 1070000\n",
      "Processing rows 1070000 to 1075000\n",
      "Processing rows 1075000 to 1080000\n",
      "Processing rows 1080000 to 1085000\n",
      "Processing rows 1085000 to 1090000\n",
      "Processing rows 1090000 to 1095000\n",
      "Processing rows 1095000 to 1100000\n",
      "Processing rows 1100000 to 1105000\n",
      "Processing rows 1105000 to 1110000\n",
      "Processing rows 1110000 to 1115000\n",
      "Processing rows 1115000 to 1120000\n",
      "Processing rows 1120000 to 1125000\n",
      "Processing rows 1125000 to 1130000\n",
      "Processing rows 1130000 to 1135000\n",
      "Processing rows 1135000 to 1140000\n",
      "Processing rows 1140000 to 1145000\n",
      "Processing rows 1145000 to 1150000\n",
      "Processing rows 1150000 to 1155000\n",
      "Processing rows 1155000 to 1160000\n",
      "Processing rows 1160000 to 1165000\n",
      "Processing rows 1165000 to 1170000\n",
      "Processing rows 1170000 to 1175000\n",
      "Processing rows 1175000 to 1180000\n",
      "Processing rows 1180000 to 1185000\n",
      "Processing rows 1185000 to 1190000\n",
      "Processing rows 1190000 to 1195000\n",
      "Processing rows 1195000 to 1200000\n",
      "Processing rows 1200000 to 1205000\n",
      "Processing rows 1205000 to 1210000\n",
      "Processing rows 1210000 to 1215000\n",
      "Processing rows 1215000 to 1220000\n",
      "Processing rows 1220000 to 1225000\n",
      "Processing rows 1225000 to 1230000\n",
      "Processing rows 1230000 to 1235000\n",
      "Processing rows 1235000 to 1240000\n",
      "Processing rows 1240000 to 1245000\n",
      "Processing rows 1245000 to 1250000\n",
      "Processing rows 1250000 to 1255000\n",
      "Processing rows 1255000 to 1260000\n",
      "Processing rows 1260000 to 1265000\n",
      "Processing rows 1265000 to 1270000\n",
      "Processing rows 1270000 to 1275000\n",
      "Processing rows 1275000 to 1280000\n",
      "Processing rows 1280000 to 1285000\n",
      "Processing rows 1285000 to 1290000\n",
      "Processing rows 1290000 to 1295000\n",
      "Processing rows 1295000 to 1300000\n",
      "Processing rows 1300000 to 1305000\n",
      "Processing rows 1305000 to 1310000\n",
      "Processing rows 1310000 to 1315000\n",
      "Processing rows 1315000 to 1320000\n",
      "Processing rows 1320000 to 1325000\n",
      "Processing rows 1325000 to 1330000\n",
      "Processing rows 1330000 to 1335000\n",
      "Processing rows 1335000 to 1340000\n",
      "Processing rows 1340000 to 1345000\n",
      "Processing rows 1345000 to 1350000\n",
      "Processing rows 1350000 to 1355000\n",
      "Processing rows 1355000 to 1360000\n",
      "Processing rows 1360000 to 1365000\n",
      "Processing rows 1365000 to 1370000\n",
      "Processing rows 1370000 to 1375000\n",
      "Processing rows 1375000 to 1380000\n",
      "Processing rows 1380000 to 1385000\n",
      "Processing rows 1385000 to 1390000\n",
      "Processing rows 1390000 to 1395000\n",
      "Processing rows 1395000 to 1400000\n",
      "Processing rows 1400000 to 1405000\n",
      "Processing rows 1405000 to 1410000\n",
      "Processing rows 1410000 to 1415000\n",
      "Processing rows 1415000 to 1420000\n",
      "Processing rows 1420000 to 1425000\n",
      "Processing rows 1425000 to 1430000\n",
      "Processing rows 1430000 to 1435000\n",
      "Processing rows 1435000 to 1440000\n",
      "Processing rows 1440000 to 1445000\n",
      "Processing rows 1445000 to 1450000\n",
      "Processing rows 1450000 to 1455000\n",
      "Processing rows 1455000 to 1460000\n",
      "Processing rows 1460000 to 1465000\n",
      "Processing rows 1465000 to 1470000\n",
      "Processing rows 1470000 to 1475000\n",
      "Processing rows 1475000 to 1480000\n",
      "Processing rows 1480000 to 1485000\n",
      "Processing rows 1485000 to 1490000\n",
      "Processing rows 1490000 to 1495000\n",
      "Processing rows 1495000 to 1500000\n",
      "Processing rows 1500000 to 1505000\n",
      "Processing rows 1505000 to 1510000\n",
      "Processing rows 1510000 to 1515000\n",
      "Processing rows 1515000 to 1520000\n",
      "Processing rows 1520000 to 1525000\n",
      "Processing rows 1525000 to 1530000\n",
      "Processing rows 1530000 to 1535000\n",
      "Processing rows 1535000 to 1540000\n",
      "Processing rows 1540000 to 1545000\n",
      "Processing rows 1545000 to 1550000\n",
      "Processing rows 1550000 to 1555000\n",
      "Processing rows 1555000 to 1560000\n",
      "Processing rows 1560000 to 1565000\n",
      "Processing rows 1565000 to 1570000\n",
      "Processing rows 1570000 to 1575000\n",
      "Processing rows 1575000 to 1580000\n",
      "Processing rows 1580000 to 1585000\n",
      "Processing rows 1585000 to 1590000\n",
      "Processing rows 1590000 to 1595000\n",
      "Processing rows 1595000 to 1600000\n",
      "Processing rows 1600000 to 1605000\n",
      "Processing rows 1605000 to 1610000\n",
      "Processing rows 1610000 to 1615000\n",
      "Processing rows 1615000 to 1620000\n",
      "Processing rows 1620000 to 1625000\n",
      "Processing rows 1625000 to 1630000\n",
      "Processing rows 1630000 to 1635000\n",
      "Processing rows 1635000 to 1640000\n",
      "Processing rows 1640000 to 1645000\n",
      "Processing rows 1645000 to 1650000\n",
      "Processing rows 1650000 to 1655000\n",
      "Processing rows 1655000 to 1660000\n",
      "Processing rows 1660000 to 1665000\n",
      "Processing rows 1665000 to 1670000\n",
      "Processing rows 1670000 to 1675000\n",
      "Processing rows 1675000 to 1680000\n",
      "Processing rows 1680000 to 1685000\n",
      "Processing rows 1685000 to 1690000\n",
      "Processing rows 1690000 to 1695000\n",
      "Processing rows 1695000 to 1700000\n",
      "Processing rows 1700000 to 1705000\n",
      "Processing rows 1705000 to 1710000\n",
      "Processing rows 1710000 to 1715000\n",
      "Processing rows 1715000 to 1720000\n",
      "Processing rows 1720000 to 1725000\n",
      "Processing rows 1725000 to 1730000\n",
      "Processing rows 1730000 to 1735000\n",
      "Processing rows 1735000 to 1740000\n",
      "Processing rows 1740000 to 1745000\n",
      "Processing rows 1745000 to 1750000\n",
      "Processing rows 1750000 to 1755000\n",
      "Processing rows 1755000 to 1760000\n",
      "Processing rows 1760000 to 1765000\n",
      "Processing rows 1765000 to 1770000\n",
      "Processing rows 1770000 to 1775000\n",
      "Processing rows 1775000 to 1780000\n",
      "Processing rows 1780000 to 1785000\n",
      "Processing rows 1785000 to 1790000\n",
      "Processing rows 1790000 to 1795000\n",
      "Processing rows 1795000 to 1800000\n",
      "Processing rows 1800000 to 1805000\n",
      "Processing rows 1805000 to 1810000\n",
      "Processing rows 1810000 to 1815000\n",
      "Processing rows 1815000 to 1820000\n",
      "Processing rows 1820000 to 1825000\n",
      "Processing rows 1825000 to 1830000\n",
      "Processing rows 1830000 to 1835000\n",
      "Processing rows 1835000 to 1840000\n",
      "Processing rows 1840000 to 1845000\n",
      "Processing rows 1845000 to 1850000\n",
      "Processing rows 1850000 to 1855000\n",
      "Processing rows 1855000 to 1860000\n",
      "Processing rows 1860000 to 1865000\n",
      "Processing rows 1865000 to 1870000\n",
      "Processing rows 1870000 to 1875000\n",
      "Processing rows 1875000 to 1880000\n",
      "Processing rows 1880000 to 1885000\n",
      "Processing rows 1885000 to 1890000\n",
      "Processing rows 1890000 to 1895000\n",
      "Processing rows 1895000 to 1900000\n",
      "Processing rows 1900000 to 1905000\n",
      "Processing rows 1905000 to 1910000\n",
      "Processing rows 1910000 to 1915000\n",
      "Processing rows 1915000 to 1920000\n",
      "Processing rows 1920000 to 1925000\n",
      "Processing rows 1925000 to 1930000\n",
      "Processing rows 1930000 to 1935000\n",
      "Processing rows 1935000 to 1940000\n",
      "Processing rows 1940000 to 1945000\n",
      "Processing rows 1945000 to 1950000\n",
      "Processing rows 1950000 to 1955000\n",
      "Processing rows 1955000 to 1960000\n",
      "Processing rows 1960000 to 1965000\n",
      "Processing rows 1965000 to 1970000\n",
      "Processing rows 1970000 to 1975000\n",
      "Processing rows 1975000 to 1980000\n",
      "Processing rows 1980000 to 1985000\n",
      "Processing rows 1985000 to 1990000\n",
      "Processing rows 1990000 to 1995000\n",
      "Processing rows 1995000 to 2000000\n",
      "Processing rows 2000000 to 2005000\n",
      "Processing rows 2005000 to 2010000\n",
      "Processing rows 2010000 to 2015000\n",
      "Processing rows 2015000 to 2020000\n",
      "Processing rows 2020000 to 2025000\n",
      "Processing rows 2025000 to 2030000\n",
      "Processing rows 2030000 to 2035000\n",
      "Processing rows 2035000 to 2040000\n",
      "Processing rows 2040000 to 2045000\n",
      "Processing rows 2045000 to 2050000\n",
      "Processing rows 2050000 to 2055000\n",
      "Processing rows 2055000 to 2060000\n",
      "Processing rows 2060000 to 2065000\n",
      "Processing rows 2065000 to 2070000\n",
      "Processing rows 2070000 to 2075000\n",
      "Processing rows 2075000 to 2080000\n",
      "Processing rows 2080000 to 2085000\n",
      "Processing rows 2085000 to 2090000\n",
      "Processing rows 2090000 to 2095000\n",
      "Processing rows 2095000 to 2100000\n",
      "Processing rows 2100000 to 2105000\n",
      "Processing rows 2105000 to 2110000\n",
      "Processing rows 2110000 to 2115000\n",
      "Processing rows 2115000 to 2120000\n",
      "Processing rows 2120000 to 2125000\n",
      "Processing rows 2125000 to 2130000\n",
      "Processing rows 2130000 to 2135000\n",
      "Processing rows 2135000 to 2140000\n",
      "Processing rows 2140000 to 2145000\n",
      "Processing rows 2145000 to 2150000\n",
      "Processing rows 2150000 to 2155000\n",
      "Processing rows 2155000 to 2160000\n",
      "Processing rows 2160000 to 2165000\n",
      "Processing rows 2165000 to 2170000\n",
      "Processing rows 2170000 to 2175000\n",
      "Processing rows 2175000 to 2180000\n",
      "Processing rows 2180000 to 2185000\n",
      "Processing rows 2185000 to 2190000\n",
      "Processing rows 2190000 to 2195000\n",
      "Processing rows 2195000 to 2200000\n",
      "Processing rows 2200000 to 2205000\n",
      "Processing rows 2205000 to 2210000\n",
      "Processing rows 2210000 to 2215000\n",
      "Processing rows 2215000 to 2220000\n",
      "Processing rows 2220000 to 2225000\n",
      "Processing rows 2225000 to 2230000\n",
      "Processing rows 2230000 to 2235000\n",
      "Processing rows 2235000 to 2240000\n",
      "Processing rows 2240000 to 2244869\n",
      "Finished processing matrix size: (2244869, 2244869)\n"
     ]
    }
   ],
   "source": [
    "k = 1.1\n",
    "total_occurrences = np.sum(cooccurrence_matrix)\n",
    "p_i = np.sum(cooccurrence_matrix, axis=1) / total_occurrences\n",
    "p_i = np.asarray(p_i).flatten()\n",
    "p_ij = cooccurrence_matrix / total_occurrences\n",
    "\n",
    "def compute_pmi_for_chunk(start_row, end_row):\n",
    "    # Lists to store data for this chunk\n",
    "    pmi_data_chunk = []\n",
    "    row_indices_chunk = []\n",
    "    col_indices_chunk = []\n",
    "\n",
    "    for i in range(start_row, end_row):\n",
    "        for data_idx in range(p_ij.indptr[i], p_ij.indptr[i + 1]):\n",
    "            j = p_ij.indices[data_idx]\n",
    "\n",
    "            if p_ij.data[data_idx] > 0:  # Avoid log(0)\n",
    "                original_pmi = np.log2(p_ij.data[data_idx] / (p_i[i] * p_i[j]))\n",
    "                pmi_score = original_pmi - (-(k - 1) * np.log2(p_ij.data[data_idx]))\n",
    "                if pmi_score < 0:\n",
    "                    pmi_data_chunk.append(pmi_score)\n",
    "                    row_indices_chunk.append(i - start_row)  # Adjust the row index relative to the chunk\n",
    "                    col_indices_chunk.append(j)\n",
    "                \n",
    "    return pmi_data_chunk, row_indices_chunk, col_indices_chunk\n",
    "\n",
    "if save:\n",
    "    # Define chunk size\n",
    "    chunk_size = 5000\n",
    "    saved_chunk_files = []\n",
    "    formatted_time = datetime.now().strftime('%H_%d_%m_%Y')\n",
    "\n",
    "    for start in range(0, cooccurrence_matrix.shape[0], chunk_size):\n",
    "        end = min(start + chunk_size, cooccurrence_matrix.shape[0])\n",
    "        print(f\"Processing rows {start} to {end}\")\n",
    "        \n",
    "        data, row, col = compute_pmi_for_chunk(start, end)\n",
    "        chunk_matrix = csr_matrix((data, (row, col)), shape=(end-start, cooccurrence_matrix.shape[1]), dtype=np.float64)\n",
    "\n",
    "        save_path = os.path.join('song_data/pmi/12_16_08_2023_1.1', f'{formatted_time}_{start}_{end}_{k}_pmi_matrix.gz')\n",
    "        with gzip.open(save_path, 'wb') as f:\n",
    "            pickle.dump(chunk_matrix, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        saved_chunk_files.append(save_path)\n",
    "\n",
    "    # Combining saved chunks\n",
    "    chunks = []\n",
    "    for chunk_file in saved_chunk_files:\n",
    "        with gzip.open(chunk_file, 'rb') as f:\n",
    "            chunks.append(pickle.load(f))\n",
    "\n",
    "    pmi_matrix = vstack(chunks, format='csr')\n",
    "    print(f'Finished processing matrix size: {pmi_matrix.shape}')\n",
    "\n",
    "else:\n",
    "    chunk_size = 5000\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, cooccurrence_matrix.shape[0], chunk_size):\n",
    "        end = min(start + chunk_size, cooccurrence_matrix.shape[0])\n",
    "        print(f\"Processing rows {start} to {end}\")\n",
    "        \n",
    "        data, row, col = compute_pmi_for_chunk(start, end)\n",
    "        chunk_matrix = csr_matrix((data, (row, col)), shape=(end-start, cooccurrence_matrix.shape[1]), dtype=np.float64)\n",
    "        chunks.append(chunk_matrix)\n",
    "        \n",
    "    pmi_matrix = vstack(chunks, format='csr')\n",
    "    print(f'Finished processing matrix size: {pmi_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing matrix size: (2244869, 2244869)\n"
     ]
    }
   ],
   "source": [
    "# to load pmi_matrix from storage\n",
    "\n",
    "folder_path = 'song_data/pmi'  # Replace with the path to your folder\n",
    "saved_chunk_files = [folder_path + '/' + f for f in os.listdir(folder_path) if \"15_08_2023\" in f]\n",
    "saved_chunk_files = sorted(saved_chunk_files, key = lambda x: int(x.split('_')[5]))\n",
    "chunks = []\n",
    "for chunk_file in saved_chunk_files:\n",
    "    with gzip.open(chunk_file, 'rb') as f:\n",
    "        chunks.append(pickle.load(f))\n",
    "\n",
    "pmi_matrix = vstack(chunks, format='csr')\n",
    "print(f'Finished processing matrix size: {pmi_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_playlist_vector(playlist_songs, song_indices, num_songs):\n",
    "    if not playlist_songs:\n",
    "        return csr_matrix(np.ones((1, num_songs)))\n",
    "    \n",
    "    indices = [song_indices[song] for song in playlist_songs]\n",
    "    data = np.ones(len(indices))\n",
    "    indptr = np.array([0, len(indices)])\n",
    "    return csr_matrix((data, indices, indptr), shape=(1, num_songs))\n",
    "\n",
    "def compute_scores(user_vector, pmi_matrix):\n",
    "    s = time.time()\n",
    "    scores = user_vector @ pmi_matrix\n",
    "    e = time.time()\n",
    "    print(e - s)\n",
    "    return scores.toarray()[0]\n",
    "\n",
    "# def get_top_recommendations(scores, song_data_map, song_indices, n=10):\n",
    "#     s = time.time()\n",
    "#     top_indices = np.argsort(scores)[-n:][::-1]\n",
    "#     top_songs = [song_data_map[list(song_indices)[i]] for i in top_indices]\n",
    "#     e = time.time()\n",
    "#     print(e - s)\n",
    "#     return top_songs\n",
    "\n",
    "def get_top_recommendations(scores, song_data_map, song_indices, n=10):\n",
    "    s = time.time()\n",
    "    \n",
    "    # Get the top n indices without sorting the entire array\n",
    "    top_indices = np.argpartition(scores, -n)[-n:]\n",
    "    # Now, sort only the top n indices\n",
    "    top_indices_sorted = top_indices[np.argsort(scores[top_indices])][::-1]\n",
    "    \n",
    "    song_indices_list = list(song_indices.keys())\n",
    "    top_songs = [song_data_map[song_indices_list[i]] for i in top_indices_sorted]\n",
    "    \n",
    "    e = time.time()\n",
    "    print(e - s)\n",
    "    return top_songs\n",
    "\n",
    "def recommend_songs_pmi(user_playlist, song_indices, pmi_matrix, song_data_map, n=10):\n",
    "    user_vector = user_playlist_vector(user_playlist, song_indices, num_songs)\n",
    "    scores = compute_scores(user_vector, pmi_matrix)\n",
    "    return get_top_recommendations(scores, song_data_map, song_indices, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongRecommendationApp(tk.Tk):\n",
    "    def __init__(self, song_data_map):\n",
    "        super().__init__()\n",
    "\n",
    "        default_font = font.nametofont(\"TkDefaultFont\")\n",
    "        default_font.configure(family=\"Courier\")\n",
    "\n",
    "        self.song_data_map = song_data_map\n",
    "        self.uri_map = {self.format_song_display(song_info): uri for uri, song_info in song_data_map.items()}\n",
    "        self.playlist_data = []  # Store song data for sorting\n",
    "\n",
    "        # Filter Frame\n",
    "        self.filter_frame = ttk.Frame(self)\n",
    "        self.filter_frame.pack(pady=10)\n",
    "\n",
    "        # Label and Entry for Song\n",
    "        self.song_label = ttk.Label(self.filter_frame, text=\"Song\")\n",
    "        self.song_label.grid(row=0, column=0, padx=5)\n",
    "        self.song_entry = ttk.Entry(self.filter_frame)\n",
    "        self.song_entry.grid(row=1, column=0, padx=5)\n",
    "\n",
    "        # Label and Entry for Artist\n",
    "        self.artist_label = ttk.Label(self.filter_frame, text=\"Artist\")\n",
    "        self.artist_label.grid(row=0, column=1, padx=5)\n",
    "        self.artist_entry = ttk.Entry(self.filter_frame)\n",
    "        self.artist_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "        # Label and Entry for Album\n",
    "        self.album_label = ttk.Label(self.filter_frame, text=\"Album\")\n",
    "        self.album_label.grid(row=0, column=2, padx=5)\n",
    "        self.album_entry = ttk.Entry(self.filter_frame)\n",
    "        self.album_entry.grid(row=1, column=2, padx=5)\n",
    "\n",
    "        # Debounce logic\n",
    "        self.last_time = time.time()\n",
    "\n",
    "        self.search_button = ttk.Button(self.filter_frame, text=\"Search\", command=self.display_search_results)\n",
    "        self.search_button.grid(row=2, columnspan=3, pady=10)\n",
    "\n",
    "        width = 200\n",
    "        # Songs Listbox\n",
    "        self.songs_listbox = tk.Listbox(self, selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.songs_listbox.pack(pady=10)\n",
    "\n",
    "        # Drag & Drop functionality\n",
    "        self.songs_listbox.bind('<<ListboxSelect>>', self.add_to_playlist)\n",
    "\n",
    "        # Playlist Listbox\n",
    "        self.playlist_listbox = tk.Listbox(self, bg=\"lightblue\", selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.playlist_listbox.pack(pady=10)\n",
    "\n",
    "        # Number of recommendations\n",
    "        self.n_label = ttk.Label(self, text=\"Number of Recommendations:\")\n",
    "        self.n_label.pack(pady=5)\n",
    "        self.n_entry = ttk.Entry(self)\n",
    "        self.n_entry.pack(pady=5)\n",
    "\n",
    "        # Button to generate recommendations\n",
    "        self.btn_recommend = ttk.Button(self, text=\"Generate Recommendations\", command=self.generate_recommendations)\n",
    "        self.btn_recommend.pack(pady=10)\n",
    "\n",
    "        # Recommendations Listbox\n",
    "        self.recommendations_listbox = tk.Listbox(self, bg=\"lightgreen\", selectmode=tk.SINGLE, width=width, font=(\"Courier\", 10))\n",
    "        self.recommendations_listbox.pack(pady=10)\n",
    "\n",
    "        self.recommendations_listbox.bind('<Double-Button-1>', self.open_in_spotify)\n",
    "\n",
    "        self.btn_refresh = ttk.Button(self, text=\"Refresh\", command=self.refresh)\n",
    "        self.btn_refresh.pack(pady=10)\n",
    "    \n",
    "    def refresh(self):\n",
    "        # Clear all fields\n",
    "        self.song_entry.delete(0, tk.END)\n",
    "        self.artist_entry.delete(0, tk.END)\n",
    "        self.album_entry.delete(0, tk.END)\n",
    "        self.n_entry.delete(0, tk.END)\n",
    "        \n",
    "        # Clear listboxes\n",
    "        self.songs_listbox.delete(0, tk.END)\n",
    "        self.playlist_listbox.delete(0, tk.END)\n",
    "        self.recommendations_listbox.delete(0, tk.END)\n",
    "\n",
    "    def open_in_spotify(self, event):\n",
    "            selected_index = self.recommendations_listbox.curselection()\n",
    "            if selected_index:\n",
    "                selected_song = self.recommendations_listbox.get(selected_index)\n",
    "                song_uri = self.uri_map[selected_song]\n",
    "                webbrowser.open(f\"https://open.spotify.com/track/{song_uri}\")\n",
    "                \n",
    "    def display_search_results(self):\n",
    "        song_query = self.song_entry.get().lower()\n",
    "        artist_query = self.artist_entry.get().lower()\n",
    "        album_query = self.album_entry.get().lower()\n",
    "\n",
    "        self.songs_listbox.delete(0, tk.END)\n",
    "        results = []  # Store the filtered results first\n",
    "\n",
    "        for uri, song_info in self.song_data_map.items():\n",
    "            if song_query in song_info['song_name'].lower() and artist_query in song_info['artist_name'].lower() and album_query in song_info['album_name'].lower():\n",
    "                display_name = self.format_song_display(song_info)\n",
    "                results.append(display_name)\n",
    "\n",
    "        # Sort by album name\n",
    "        results.sort(key=lambda x: self.song_data_map[self.uri_map[x]]['album_name'])\n",
    "\n",
    "        # Display the sorted results\n",
    "        for display_name in results:\n",
    "            self.songs_listbox.insert(tk.END, display_name)\n",
    "\n",
    "        if len(results) > 300:  # If you want to limit the displayed results\n",
    "            self.songs_listbox.delete(301, tk.END)\n",
    "\n",
    "    def format_song_display(self, song_info):\n",
    "        formatted_str = \"{:<65}{:<35}{:<35}\"\n",
    "        f_string = formatted_str.format(song_info['song_name'], song_info['artist_name'], song_info['album_name'])\n",
    "        return f_string\n",
    "\n",
    "    def add_to_playlist(self, event):\n",
    "        selected_index = self.songs_listbox.curselection()\n",
    "        if selected_index:  # This checks if there's any selection at all\n",
    "            selected_song = self.songs_listbox.get(selected_index)\n",
    "            if selected_song not in self.playlist_listbox.get(0, tk.END):  # Prevent duplicates\n",
    "                self.playlist_listbox.insert(tk.END, selected_song)\n",
    "\n",
    "    def generate_recommendations(self):\n",
    "        s = time.time()\n",
    "        playlist_display_names = list(self.playlist_listbox.get(0, tk.END))\n",
    "        playlist_uris = [self.uri_map[display_name] for display_name in playlist_display_names]  # Extract URIs\n",
    "\n",
    "        n = int(self.n_entry.get())\n",
    "        recommended_songs = recommend_songs_pmi(playlist_uris, song_indices, pmi_matrix, song_data_map, n)\n",
    "\n",
    "        self.recommendations_listbox.delete(0, tk.END)\n",
    "        for song in recommended_songs:\n",
    "            formatted_song = self.format_song_display(song)\n",
    "            self.recommendations_listbox.insert(tk.END, formatted_song)\n",
    "        e = time.time()\n",
    "        print(e - s)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = SongRecommendationApp(song_data_map)\n",
    "    app.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
